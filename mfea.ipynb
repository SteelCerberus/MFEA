{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bf3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:36.519180Z",
     "start_time": "2023-09-02T01:15:36.496668Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib as mpl\n",
    "\n",
    "import pandas_datareader as web\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f9a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.017057Z",
     "start_time": "2023-09-02T01:15:36.995030Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_spy_extended():\n",
    "    # This method is how I generated the daily data from 1885 to present. I'm leaving it in here\n",
    "    # so people can check my work. If you want any of the datasets used here let me know.\n",
    "    \n",
    "    df_shiller_dividend = pd.read_csv('data/dividend_data_shiller.csv').dropna()\n",
    "\n",
    "    df_gspc = Backtest.load_additional_dfs('^GSPC')\n",
    "    df_gspc['Close (Sim)'] = df_gspc['Close'].copy()\n",
    "    df_gspc_monthly = df_gspc.loc[df_gspc.groupby(df_gspc.index.to_period('M')) \\\n",
    "                                            .apply(lambda x: x.index.max())][:-1]\n",
    "    df_gspc = df_gspc.loc[df_gspc.index <= datetime(2022, 11, 1)]\n",
    "\n",
    "    BASE_PRICE = 16.66\n",
    "    previous_close = BASE_PRICE\n",
    "    previous_tr = BASE_PRICE\n",
    "    previous_index = df_gspc.index[0]\n",
    "\n",
    "    for index, row in df_gspc.loc[datetime(1950, 1, 4):df_gspc_monthly.index[-3]].iterrows():\n",
    "        dividend = 0\n",
    "        if index in df_gspc_monthly.index:\n",
    "            dividend = df_shiller_dividend.loc[(df_shiller_dividend['Year'] == index.year) \n",
    "                                               & (df_shiller_dividend['Month'] == index.month), 'dividend'].iloc[0]\n",
    "\n",
    "        new_tr = previous_tr * (row['Close'] / ([previous_close - dividend]))\n",
    "        df_gspc.loc[index, 'Close (Sim)'] = new_tr\n",
    "\n",
    "        previous_close = df_gspc.loc[index, 'Close']\n",
    "        previous_tr = new_tr\n",
    "        previous_index = index\n",
    "\n",
    "    df_gspc = df_gspc[['Close (Sim)']]\n",
    "\n",
    "    df_vfinx_gspc = Backtest.load_additional_dfs('SPY')\n",
    "    df_vfinx_gspc = df_vfinx_gspc.pct_change().merge(df_gspc.pct_change(), \n",
    "                                                     how='outer',\n",
    "                                                     left_index=True,\n",
    "                                                     right_index=True)\n",
    "\n",
    "    df_vfinx_gspc['Close'] = df_vfinx_gspc[['Close', 'Close (Sim)']].bfill(axis=1).iloc[:, 0]\n",
    "    df_vfinx_gspc['Close'] = (df_vfinx_gspc['Close']+1).cumprod()\n",
    "    df_vfinx_gspc.loc[df_vfinx_gspc.index[0], 'Close'] = 1\n",
    "    df_vfinx_gspc = df_vfinx_gspc[['Close']]\n",
    "\n",
    "    df_spy_extended = pd.read_csv('data/extended_spy_data.csv', index_col = False)\n",
    "    df_spy_extended['Date'] = pd.to_datetime(df_spy_extended['Date'], format='%Y%m%d')\n",
    "    df_spy_extended = df_spy_extended.set_index('Date')\n",
    "    df_spy_extended['Close'] = (df_spy_extended['Daily Return'] + 1).cumprod()\n",
    "\n",
    "    df_spy_extended = df_spy_extended.pct_change().merge(df_vfinx_gspc.pct_change(), \n",
    "                                                         how='outer', \n",
    "                                                         left_index=True, \n",
    "                                                         right_index=True)\n",
    "    df_spy_extended['Close'] = df_spy_extended[['Close_x', 'Close_y']].bfill(axis=1).iloc[:, 0]\n",
    "    df_spy_extended['Close'] = (df_spy_extended['Close']+1).cumprod()\n",
    "    df_spy_extended.loc[df_spy_extended.index[0], 'Close'] = 1\n",
    "    \n",
    "    df_shiller = pd.read_excel('data/ie_data.xls', sheet_name='Data', header=7, nrows=1831, dtype={'Date': 'string'})\n",
    "    df_shiller['Date'] = df_shiller['Date'].apply(lambda x: x.replace('.', '-'))\n",
    "    df_shiller.index = df_shiller['Date'].apply(lambda x: re.sub('-1$', '-10', x))\n",
    "    df_shiller.index = pd.to_datetime(df_shiller.index)\n",
    "    df_shiller = df_shiller.loc[:, ['Rate GS10']]\n",
    "    df_shiller['Rate GS10'] -= 1  # Adjust to make closer to risk free rate. This does have a pretty big impact on results\n",
    "\n",
    "    df_tbill = web.DataReader('TB3MS', 'fred', datetime(1934, 1, 1))\n",
    "    df_shiller = df_shiller.merge(df_tbill, how='outer', left_index=True, right_index=True)\n",
    "    df_shiller['RFR'] = df_shiller[['TB3MS', 'Rate GS10']].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "    df_spy_extended = df_spy_extended.merge(df_shiller, how='left', left_index=True, right_index=True)\n",
    "    df_spy_extended['Risk Free Rate'] = df_spy_extended['RFR'].interpolate()\n",
    "    df_spy_extended = df_spy_extended[['Close', 'Risk Free Rate']].dropna()\n",
    "\n",
    "    total_costs = 0.000945 / 252\n",
    "    df_spy_extended.loc[Backtest.load_additional_dfs('SPY').index,\n",
    "                        'Pct Change'] = df_spy_extended['Close'].pct_change()\n",
    "    df_spy_extended.loc[~df_spy_extended.index.isin(Backtest.load_additional_dfs('SPY').index), \n",
    "                        'Pct Change'] = df_spy_extended['Close'].pct_change() - total_costs\n",
    "\n",
    "    df_spy_extended['Close'] = (df_spy_extended['Pct Change'] + 1).cumprod()\n",
    "    df_spy_extended.loc[df_spy_extended.index[0], 'Close'] = 1\n",
    "    df_spy_extended = df_spy_extended[['Close', 'Risk Free Rate']].dropna()\n",
    "\n",
    "    return df_spy_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a7efb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.048053Z",
     "start_time": "2023-09-02T01:15:37.038841Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_additional_dfs(ticker):\n",
    "    df = yf.Ticker(ticker).history(period='max')\n",
    "    df = df[['Close']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c3e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.078674Z",
     "start_time": "2023-09-02T01:15:37.069320Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_us_market_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.set_index('Date')\n",
    "    \n",
    "    df_spy = load_additional_dfs('SPY')\n",
    "    df_spy = df_spy.pct_change().merge(df.pct_change(), how='outer', left_index=True, right_index=True)\n",
    "    df_spy['Close'] = df_spy[['Close_y', 'Close_x']].bfill(axis=1).iloc[:, 0]\n",
    "    df_spy['Close'] = (df_spy['Close'] + 1).cumprod()\n",
    "    df_spy.loc[df_spy.index[0], 'Close'] = 1\n",
    "    df_spy = df_spy[['Close']]\n",
    "    \n",
    "    df_tbill = web.DataReader('TB3MS', 'fred', datetime(1934, 1, 1))\n",
    "    df_tbill = df_tbill.merge(df, how='outer', left_index=True, right_index=True)\n",
    "    df_tbill['RFR'] = df_tbill[['Risk Free Rate', 'TB3MS']].bfill(axis=1).iloc[:, 0]\n",
    "    df_tbill = df_tbill[['RFR']]\n",
    "    \n",
    "    df_us_data = df_spy.merge(df_tbill, how='left', left_index=True, right_index=True)\n",
    "    df_us_data['Risk Free Rate'] = df_us_data['RFR'].interpolate()\n",
    "    \n",
    "    return df_us_data[['Close', 'Risk Free Rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d597530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.109597Z",
     "start_time": "2023-09-02T01:15:37.101427Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Source: https://gist.github.com/tclements/7452c0fac3e66e08b886300b4e24e687\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5a9cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.141265Z",
     "start_time": "2023-09-02T01:15:37.132260Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def find_middle_month(dates):\n",
    "    return min(dates, key=lambda x: abs(x - datetime(dates[0].year, dates[0].month, 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8237e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.188406Z",
     "start_time": "2023-09-02T01:15:37.162860Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_df_data(df_spy, \n",
    "                 vol_lookback,\n",
    "                 mom_lookback, \n",
    "                 short_mom_lookback,\n",
    "                 tbill_lookback, \n",
    "                 use_lin_reg,\n",
    "                 use_sma, \n",
    "                 monthly, \n",
    "                 use_tbill_data):\n",
    "    # If use_tbill_data is not set, there must be a Risk Free Rate column in df_spy\n",
    "    df_spy = df_spy.copy()\n",
    "        \n",
    "    df_spy['Pct Change'] = df_spy['Close'].pct_change()\n",
    "\n",
    "    stds = np.std(rolling_window(df_spy['Pct Change'].dropna().to_numpy(), \n",
    "                                                          vol_lookback), axis=1, ddof=1) * (252 ** 0.5)\n",
    "    stds = np.insert(stds, 0, [None] * vol_lookback)\n",
    "    df_spy['Volatility'] = stds\n",
    "\n",
    "    df_spy['Skew'] = df_spy['Close'].rolling(mom_lookback).skew()\n",
    "    if use_sma:\n",
    "        x = rolling_window(df_spy['Close'].to_numpy(), mom_lookback)\n",
    "        momentums = x[:, -1] / np.mean(x, axis = 1)\n",
    "        momentums = np.insert(momentums, 0, [None] * (mom_lookback - 1))\n",
    "        df_spy['Momentum'] = momentums\n",
    "        \n",
    "        x = rolling_window(df_spy['Close'].to_numpy(), short_mom_lookback)\n",
    "        momentums = x[:, -1] / np.mean(x, axis = 1)\n",
    "        momentums = np.insert(momentums, 0, [None] * (short_mom_lookback - 1))\n",
    "        df_spy['Short Momentum'] = momentums\n",
    "    else:\n",
    "        x = rolling_window(df_spy['Close'].to_numpy(), mom_lookback)\n",
    "        momentums = x[:, -1] / x[:, 0]\n",
    "        momentums = np.insert(momentums, 0, [None] * (mom_lookback - 1))\n",
    "        df_spy['Momentum'] = momentums\n",
    "        \n",
    "        x = rolling_window(df_spy['Close'].to_numpy(), short_mom_lookback)\n",
    "        momentums = x[:, -1] / x[:, 0]\n",
    "        momentums = np.insert(momentums, 0, [None] * (short_mom_lookback - 1))\n",
    "        df_spy['Short Momentum'] = momentums\n",
    "\n",
    "    df_tbill = pd.DataFrame()\n",
    "\n",
    "    if use_tbill_data:\n",
    "        df_tbill = web.DataReader('DTB3', 'fred', df_spy.index[0]).dropna()\n",
    "        df_tbill['Risk Free Rate'] = df_tbill['DTB3']\n",
    "        df_tbill = df_tbill[['Risk Free Rate']]\n",
    "        df_spy = df_spy.merge(df_tbill, how='left', left_index=True, right_index=True)\n",
    "        df_spy['Risk Free Rate'] = df_spy['Risk Free Rate'].ffill()\n",
    "\n",
    "    if use_lin_reg:\n",
    "        df_spy['Treasury Change'] = df_spy['Risk Free Rate'] \\\n",
    "                                        .rolling(tbill_lookback) \\\n",
    "                                        .apply(lambda x: linear_regression(x))\n",
    "    else:\n",
    "        x = rolling_window(df_spy['Risk Free Rate'].to_numpy(), tbill_lookback)\n",
    "        treasury_changes = x[:, -1] - x[:, 0]\n",
    "        treasury_changes = np.insert(treasury_changes, 0, [None] * (tbill_lookback - 1))\n",
    "        df_spy['Treasury Change'] = treasury_changes\n",
    "    \n",
    "    df_spy = df_spy.dropna()\n",
    "    \n",
    "    if monthly:\n",
    "        first_month_end = df_spy.groupby(df_spy.index.to_period('M')).apply(lambda x: find_middle_month(x.index))[0]\n",
    "        df_spy = df_spy.loc[df_spy.index >= first_month_end].copy()\n",
    "\n",
    "    return df_spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dae972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.203535Z",
     "start_time": "2023-09-02T01:15:37.194359Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def linear_regression(data):\n",
    "    X = data.index.values.reshape(-1, 1).astype(float)\n",
    "    Y = data.values\n",
    "    \n",
    "    linear_regressor = LinearRegression()\n",
    "    linear_regressor.fit(X, Y)\n",
    "    \n",
    "    return linear_regressor.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef496b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.234735Z",
     "start_time": "2023-09-02T01:15:37.227137Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_signals(df, lower_vol_cutoff, upper_vol_cutoff, strict_boundaries, exclude_vol, exclude_mom, monthly_calc):\n",
    "    df = df.copy()\n",
    "    vol = df['Volatility'].to_numpy()\n",
    "    mom = df['Momentum'].to_numpy()\n",
    "    short_mom = df['Short Momentum'].to_numpy()\n",
    "    treasury = df['Treasury Change'].to_numpy()\n",
    "\n",
    "    if monthly_calc:\n",
    "        df_monthly = df.copy()\n",
    "        df_monthly = df_monthly.loc[df_monthly.groupby(df_monthly.index.to_period('M')) \\\n",
    "                                        .apply(lambda x: find_middle_month(x.index))][:-1]\n",
    "        tmp = df.reset_index()\n",
    "        monthly_indices = tmp.loc[tmp['Date'].isin(df_monthly.index)]\n",
    "        monthly_indices = list(monthly_indices.index)\n",
    "        states = get_states(vol, mom, short_mom, treasury, lower_vol_cutoff, upper_vol_cutoff, \n",
    "                            strict_boundaries, exclude_vol, exclude_mom, monthly_indices)\n",
    "    else:\n",
    "        states = get_states(vol, mom, short_mom, treasury, lower_vol_cutoff, upper_vol_cutoff,\n",
    "                            strict_boundaries, exclude_vol, exclude_mom)\n",
    "    \n",
    "    df['State'] = states\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8262a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.280975Z",
     "start_time": "2023-09-02T01:15:37.256607Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_states(vol_arr, \n",
    "               mom_arr,\n",
    "               short_mom_arr,\n",
    "               treasury_arr, \n",
    "               lower_vol_cutoff, \n",
    "               upper_vol_cutoff, \n",
    "               strict_boundaries,\n",
    "               exclude_vol,\n",
    "               exclude_mom,\n",
    "               month_end_indices = None):\n",
    "    states = []\n",
    "    previous_state = ''\n",
    "    for idx in range(len(vol_arr)):\n",
    "        vol = vol_arr[idx]\n",
    "        mom = mom_arr[idx]\n",
    "        treasury = treasury_arr[idx]\n",
    "        short_mom = short_mom_arr[idx]\n",
    "        \n",
    "        if month_end_indices:\n",
    "            if idx not in month_end_indices:\n",
    "                states.append(previous_state)\n",
    "                continue\n",
    "        \n",
    "        adjusted_lower_vol_cutoff = lower_vol_cutoff\n",
    "        adjusted_upper_vol_cutoff = upper_vol_cutoff\n",
    "        adjusted_mom_cutoff = 1\n",
    "        adjusted_treasury_cutoff = 0\n",
    "        \n",
    "        if not strict_boundaries:\n",
    "            if previous_state == 'Risk On':\n",
    "                adjusted_lower_vol_cutoff += 0.01\n",
    "                adjusted_mom_cutoff -= 0.02\n",
    "            elif previous_state == 'Risk Mid':\n",
    "                adjusted_lower_vol_cutoff -= 0.01\n",
    "                adjusted_upper_vol_cutoff += 0.01\n",
    "                adjusted_mom_cutoff -= 0.02\n",
    "            elif previous_state == 'Risk Off':\n",
    "                adjusted_lower_vol_cutoff -= 0.01\n",
    "                adjusted_upper_vol_cutoff -= 0.01\n",
    "                if not USE_LINEAR_REGRESSION:\n",
    "                    adjusted_treasury_cutoff -= 0.1\n",
    "                adjusted_mom_cutoff += 0.02\n",
    "            elif previous_state == 'Risk Alt':\n",
    "                adjusted_lower_vol_cutoff -= 0.01\n",
    "                adjusted_upper_vol_cutoff -= 0.01\n",
    "                if not USE_LINEAR_REGRESSION:\n",
    "                    adjusted_treasury_cutoff += 0.1\n",
    "                adjusted_mom_cutoff += 0.02\n",
    "        \n",
    "        low_vol = exclude_vol or vol <= adjusted_lower_vol_cutoff\n",
    "        med_vol = exclude_vol or ((vol >= adjusted_lower_vol_cutoff) and (vol <= adjusted_upper_vol_cutoff))\n",
    "        \n",
    "        high_mom = exclude_mom or mom >= adjusted_mom_cutoff\n",
    "        \n",
    "        if low_vol and high_mom:\n",
    "            new_state = 'Risk On'\n",
    "        elif (low_vol or med_vol) and high_mom:\n",
    "            new_state = 'Risk Mid'\n",
    "        else:\n",
    "            if (treasury > adjusted_treasury_cutoff):\n",
    "                new_state = 'Risk Off'\n",
    "            else:\n",
    "                new_state = 'Risk Alt'\n",
    "        \n",
    "        states.append(new_state)\n",
    "        previous_state = new_state\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e0efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.577690Z",
     "start_time": "2023-09-02T01:15:37.566557Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_signal(df, plot_width, plot_height, \n",
    "                show_risk_on, show_risk_mid, show_risk_alt, show_risk_off, \n",
    "                low_vol_cutoff, upper_vol_cutoff):\n",
    "    fig, ax1 = plt.subplots(figsize=(plot_width, plot_height))\n",
    "\n",
    "    ax1.plot(df.index, df['Close'], label = 'S&P 500', c = 'black', linewidth = 2)\n",
    "\n",
    "    ax1.margins(x = 0)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('Date', fontsize = 14)\n",
    "    ax1.set_ylabel('S&P 500', fontsize = 14)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax2.plot(df.index, df['Volatility'] * 100, label = 'Volatility', c = 'blue', alpha = 0.7)\n",
    "\n",
    "    ax2.axhline(y = low_vol_cutoff * 100, label = 'Lower Vol Cutoff', c = 'green')\n",
    "    ax2.axhline(y = upper_vol_cutoff * 100, label = 'Higher Vol Cutoff', c = 'red')\n",
    "\n",
    "    if show_risk_on:\n",
    "        ax2.fill_between(df.index, 0, 1, where = (df['State'] == 'Risk On'), linewidth = 0,\n",
    "                         transform = ax1.get_xaxis_transform(), color = 'green', alpha = 0.1, label = 'Risk On')\n",
    "    if show_risk_mid:\n",
    "        ax2.fill_between(df.index, 0, 1, where = (df['State'] == 'Risk Mid'), linewidth = 0,\n",
    "                         transform = ax1.get_xaxis_transform(), color = 'blue', alpha = 0.1, label = 'Risk Mid')\n",
    "    if show_risk_alt:\n",
    "        ax2.fill_between(df.index, 0, 1, where = (df['State'] == 'Risk Alt'), linewidth = 0,\n",
    "                         transform = ax1.get_xaxis_transform(), color = 'yellow', alpha = 0.1, label = 'Risk Alt')\n",
    "    if show_risk_off:\n",
    "        ax2.fill_between(df.index, 0, 1, where = (df['State'] == 'Risk Off'), linewidth = 0,\n",
    "                         transform = ax1.get_xaxis_transform(), color = 'red', alpha = 0.1, label = 'Risk Off')\n",
    "\n",
    "    ax2.margins(x = 0)\n",
    "    ax2.set_ylabel('21 Day Volatility (Percent)', fontsize = 14)\n",
    "\n",
    "    plt.title(f'Volatility + Momentum Indicator to S&P 500, ' + \\\n",
    "                  df.dropna().index[0].strftime('%b %Y') + \\\n",
    "                  ' - ' + df.dropna().index[-1].strftime('%b %Y'), fontsize = 14)\n",
    "\n",
    "    fig.legend(loc = 'upper left', bbox_to_anchor = (0,1), bbox_transform = ax1.transAxes)\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241c617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:43:37.635595Z",
     "start_time": "2023-09-02T01:43:37.591047Z"
    },
    "code_folding": [
     1,
     6,
     41,
     65,
     76,
     101,
     112,
     133,
     151,
     171,
     192,
     202,
     204,
     216,
     227,
     229,
     233,
     237
    ]
   },
   "outputs": [],
   "source": [
    "class TickerLoader:\n",
    "    def __init__(self, df_spy, method):\n",
    "        self.df_spy = df_spy\n",
    "        self.method = method\n",
    "        \n",
    "    # Credits to Leminspector for this function\n",
    "    def underlying_to_letf(self,\n",
    "                           underlying,             # Underlying ETF\n",
    "                           s,                      # Swap exposure (usually around 1.1)\n",
    "                           E,                      # Expense ratio\n",
    "                           beta,                   # Leverage\n",
    "                           start=1,               # Starting price of LETF stock\n",
    "                           spread=0.004,           # Spread\n",
    "                           alt_risk_free_rate=None # Alternate risk free rate to use\n",
    "                           ):                      # Returns pandas Series object of LETF daily prices\n",
    "        daily_prices = underlying['Close']\n",
    "        daily_change = daily_prices.pct_change().dropna().rename('Daily Change')\n",
    "        \n",
    "        I = 0\n",
    "        if alt_risk_free_rate is None:  # Earliest date for FFR\n",
    "            I = web.DataReader('DFF', 'fred', daily_prices.index[0])/100 + spread  # LIBOR + spread\n",
    "            if I.index[-1] < daily_prices.index[-1]:\n",
    "                I.loc[daily_prices.index[-1]] = I['DFF'][-1]\n",
    "        else:\n",
    "            I = alt_risk_free_rate / 100 + spread\n",
    "            \n",
    "        total_costs = (s * (beta - 1) * I + E) / 252\n",
    "        \n",
    "        df = total_costs.join(daily_change, how='inner')\n",
    "        \n",
    "        \n",
    "        df['LETF Change'] = df['Daily Change'] * beta - df.iloc[:, 0]\n",
    "        df['LETF Price'] = start * (1 + df['LETF Change']).cumprod()\n",
    "\n",
    "        new_df = df['LETF Price'].to_frame(name='Close')\n",
    "\n",
    "        new_df.loc[underlying['Close'].dropna().index[0], 'Close'] = start\n",
    "        new_df = new_df.reindex(np.roll(new_df.index, shift=1))\n",
    "\n",
    "        return new_df\n",
    "    \n",
    "    def calculate_daily_returns_coupon_bond(self, df, n):\n",
    "        df['Return'] = 0\n",
    "        mask_zero = df.iloc[1:, 0] == 0\n",
    "        mask_not_zero = df.iloc[1:, 0] != 0\n",
    "\n",
    "        mask_zero_shifted = (df.iloc[:, 0] == 0).shift(-1)\n",
    "        mask_zero_shifted.iloc[-1] = False\n",
    "        mask_not_zero_shifted = (df.iloc[:, 0] != 0).shift(-1)\n",
    "        mask_not_zero_shifted.iloc[-1] = False\n",
    "\n",
    "        values_zero = df.iloc[1:, 0][mask_zero].values / 100\n",
    "        values_zero_previous = df.iloc[:, 0][mask_zero_shifted].values / 100\n",
    "        returns_zero = values_zero_previous * ((1 / 504) + n)\n",
    "        df.loc[df.iloc[:, 0] == 0, 'Return'] = returns_zero\n",
    "\n",
    "        val = df.iloc[1:, 0][mask_not_zero].values / 100\n",
    "        val_pre = df.iloc[:, 0][mask_not_zero_shifted].values / 100\n",
    "        returns_non_zero = ((val - val_pre) / val) * (((1 / (1 + val)) ** n) - 1) + (((val + val_pre) / 2) / 252)\n",
    "        returns_non_zero = np.insert(returns_non_zero, 0, 0)\n",
    "        df.loc[df.iloc[:, 0] != 0, 'Return'] = returns_non_zero\n",
    "\n",
    "        df.iloc[0, 1] = 0\n",
    "        df['Close'] = (df['Return'] + 1).cumprod()\n",
    "        \n",
    "    def yield_to_prices(self, df, n):\n",
    "        df = df.copy().dropna()\n",
    "        self.calculate_daily_returns_coupon_bond(df, n)\n",
    "        \n",
    "        # Columbus/Indigenous People's Day and Veteran's Day recognized by Fed, not by stock exchange\n",
    "        df_spy_subset = self.df_spy.loc[(self.df_spy.index >= df.index[0]) & (self.df_spy.index <= df.index[-1])]\n",
    "        df = df_spy_subset.merge(df, how = 'left', left_index = True, right_index = True, suffixes = ['_SPY', None])\n",
    "        df = df[['Close']].ffill().dropna()\n",
    "\n",
    "        return df.loc[:, ['Close']]\n",
    "        \n",
    "    def calculate_daily_returns_no_coupon_bond_yield(self, df, n):\n",
    "        df['Return'] = 0\n",
    "        mask_zero = df.iloc[1:, 0] == 0\n",
    "        mask_not_zero = df.iloc[1:, 0] != 0\n",
    "\n",
    "        mask_zero_shifted = (df.iloc[:, 0] == 0).shift(-1)\n",
    "        mask_zero_shifted.iloc[-1] = False\n",
    "        mask_not_zero_shifted = (df.iloc[:, 0] != 0).shift(-1)\n",
    "        mask_not_zero_shifted.iloc[-1] = False\n",
    "\n",
    "        values_zero = df.iloc[1:, 0][mask_zero].values / 100\n",
    "        values_zero_previous = df.iloc[:, 0][mask_zero_shifted].values / 100\n",
    "        returns_zero = (1 + values_zero_previous) ** n\n",
    "        df.loc[df.iloc[:, 0] == 0, 'Return'] = returns_zero\n",
    "\n",
    "        values_not_zero = df.iloc[1:, 0][mask_not_zero].values / 100\n",
    "        values_not_zero_previous = df.iloc[:, 0][mask_not_zero_shifted].values / 100\n",
    "        returns_non_zero = (((1 + values_not_zero_previous) / (1 + values_not_zero)) ** n) \\\n",
    "                                        * ((1 + values_not_zero) ** (1 / 252))\n",
    "        returns_non_zero = np.insert(returns_non_zero, 0, 0)\n",
    "        df.loc[df.iloc[:, 0] != 0, 'Return'] = returns_non_zero\n",
    "\n",
    "        df.iloc[0, 1] = 1\n",
    "        df['Close'] = df['Return'].cumprod()\n",
    "    \n",
    "    def yield_to_prices_zroz(self, df, n):\n",
    "        df = df.copy().dropna()\n",
    "        self.calculate_daily_returns_no_coupon_bond_yield(df, n)\n",
    "        \n",
    "        # Columbus/Indigenous People's Day and Veteran's Day recognized by Fed, not by stock exchange\n",
    "        df_spy_subset = self.df_spy.loc[(self.df_spy.index >= df.index[0]) & (self.df_spy.index <= df.index[-1])]\n",
    "        df = df_spy_subset.merge(df, how = 'left', left_index = True, right_index = True, suffixes = ['_SPY', None])\n",
    "        df = df[['Close']].ffill().dropna()\n",
    "\n",
    "        return df.loc[:, ['Close']]\n",
    "                \n",
    "    def load_zroz(self):\n",
    "        df_30_yield = web.DataReader('DGS30', 'fred', datetime(1977, 2, 15)).dropna()\n",
    "        if df_30_yield.index[-1] < self.df_spy.index[-1]:\n",
    "            df_30_yield.loc[self.df_spy.index[-1], 'DGS30'] = df_30_yield['DGS30'][-1]\n",
    "        df_20_yield = web.DataReader('DGS20', 'fred', datetime(1962, 1, 2)).dropna()\n",
    "\n",
    "        df_zroz_30 = self.yield_to_prices_zroz(df_30_yield, 30)\n",
    "        df_zroz_20 = self.yield_to_prices_zroz(df_20_yield, 20)\n",
    "\n",
    "        df_zroz = df_zroz_30.pct_change().merge(df_zroz_20.pct_change(), how='outer',\n",
    "                                                                 left_index=True, right_index=True)\n",
    "        df_zroz['Close'] = df_zroz[['Close_x', 'Close_y']].bfill(axis=1).iloc[:, 0]\n",
    "        df_zroz['Close'] = (df_zroz['Close']+1).cumprod()\n",
    "        df_zroz.loc[df_zroz.index[0], 'Close'] = 1\n",
    "        df_zroz = df_zroz[['Close']]\n",
    "\n",
    "        df_zroz = df_zroz[df_zroz.index.isin(self.df_spy.index)]\n",
    "        df_zroz = self.underlying_to_letf(df_zroz, s=0, E=0.0015, beta=1, spread=0)\n",
    "        \n",
    "        return df_zroz\n",
    "    \n",
    "    def load_risk_free_return(self):\n",
    "        risk_free_return = web.DataReader('DFF', 'fred', datetime(1954, 7, 1))\n",
    "        risk_free_return = risk_free_return.loc[risk_free_return.index.isin(self.df_spy.index)]\n",
    "        if risk_free_return.index[-1] < self.df_spy.index[-1]:\n",
    "            risk_free_return.loc[self.df_spy.index[-1], 'DFF'] = risk_free_return['DFF'][-1]\n",
    "        risk_free_return['Close'] = (risk_free_return['DFF'] / 100 / 252 + 1).cumprod()\n",
    "        risk_free_return = risk_free_return.loc[:, ['Close']]\n",
    "\n",
    "        df_spy_subset = self.df_spy.loc[(self.df_spy.index >= risk_free_return.index[0]) \\\n",
    "                                   & (self.df_spy.index <= risk_free_return.index[-1])]\n",
    "\n",
    "        # Columbus/Indigenous People's Day and Veteran's Day recognized by Fed, not by stock exchange\n",
    "        risk_free_return = df_spy_subset.merge(risk_free_return, how = 'left', \n",
    "                                               left_index = True, right_index = True, suffixes = ['_SPY', None])\n",
    "        risk_free_return = risk_free_return[['Close']].ffill().dropna()\n",
    "        \n",
    "        return risk_free_return\n",
    "    \n",
    "    def load_kmlm(self):\n",
    "        df = pd.read_excel(KMLM_DATA_PATH, sheet_name = 'MLM Index EV (15V) Daily', header = 2)\n",
    "        df.index = pd.to_datetime(df['Date'])\n",
    "        df['Close'] = (df['TotalReturnDtd'] + 1).cumprod()\n",
    "        df = df[['Close']]\n",
    "\n",
    "        df_spy_subset = self.df_spy.loc[(self.df_spy.index >= df.index[0]) \\\n",
    "                               & (self.df_spy.index <= datetime(2020, 12, 2))]\n",
    "        \n",
    "        df = df_spy_subset.merge(df, how = 'left', left_index = True, right_index = True, suffixes = ['_SPY', None])\n",
    "\n",
    "        df_kmlm = load_additional_dfs('KMLM')\n",
    "        df_kmlm = df_kmlm.pct_change().merge(df[['Close']].pct_change(), how='outer', left_index=True, right_index=True)\n",
    "        df_kmlm['Close'] = df_kmlm[['Close_y', 'Close_x']].bfill(axis=1).iloc[:, 0]\n",
    "        df_kmlm['Close'] = (df_kmlm['Close'] + 1).cumprod()\n",
    "        df_kmlm.loc[df_kmlm.index[0], 'Close'] = 1\n",
    "        df_kmlm = df_kmlm[['Close']]\n",
    "        \n",
    "        return df_kmlm\n",
    "    \n",
    "    def load_tmf(self):\n",
    "        df_30_yield = web.DataReader('DGS30', 'fred', datetime(1977, 2, 15)).dropna()\n",
    "        if df_30_yield.index[-1] < self.df_spy.index[-1]:\n",
    "            df_30_yield.loc[self.df_spy.index[-1], 'DGS30'] = df_30_yield['DGS30'][-1]\n",
    "        df_20_yield = web.DataReader('DGS20', 'fred', datetime(1962, 1, 2)).dropna()\n",
    "\n",
    "        df_tmf_30 = self.yield_to_prices(df_30_yield, 30)\n",
    "        df_tmf_20 = self.yield_to_prices(df_20_yield, 20)\n",
    "\n",
    "        df_tmf = df_tmf_30.pct_change().merge(df_tmf_20.pct_change(), how='outer',\n",
    "                                                                 left_index=True, right_index=True)\n",
    "        df_tmf['Close'] = df_tmf[['Close_x', 'Close_y']].bfill(axis=1).iloc[:, 0]\n",
    "        df_tmf['Close'] = (df_tmf['Close']+1).cumprod()\n",
    "        df_tmf.loc[df_tmf.index[0], 'Close'] = 1\n",
    "        df_tmf = df_tmf[['Close']]\n",
    "\n",
    "        df_tmf = df_tmf[df_tmf.index.isin(self.df_spy.index)]\n",
    "        df_tmf = self.underlying_to_letf(df_tmf, s=1.1, E=0.0106, beta=3, spread=0)\n",
    "        \n",
    "        return df_tmf\n",
    "\n",
    "    def load_precalculated_dfs(self):\n",
    "        df_sso = self.underlying_to_letf(self.df_spy, s=1.1, E=0.0088, beta=2, spread=0.005)\n",
    "        df_upro = self.underlying_to_letf(self.df_spy, s=1.1, E=0.0091, beta=3, spread=0.005)\n",
    "        risk_free_return = self.load_risk_free_return()\n",
    "        \n",
    "        # All backtests need these for plotting\n",
    "        precalculated_dfs = {'SSO': df_sso, \n",
    "                             'UPRO': df_upro,\n",
    "                             'Risk Free Return': risk_free_return}        \n",
    "            \n",
    "        if self.method in ['standard', 'no risk alt', 'rfr instead of spy', 'upro zroz']:\n",
    "            precalculated_dfs['ZROZ'] = self.load_zroz()\n",
    "        elif self.method in ['rfr instead of zroz and spy', 'rfr instead of zroz', 'dynamic', 'simple', 'sso', \n",
    "                             'upro']:\n",
    "            if 'Risk Free Rate' in self.df_spy.columns:\n",
    "                # This is the special case where we are using extended data with rfr data included in df_spy\n",
    "                # No else statement is required because SSO, UPRO, and RFR are already added to precalculated_dfs\n",
    "                df_sso = self.underlying_to_letf(self.df_spy, s=1.1, E=0.0088, beta=2, \n",
    "                                                 spread=0.005, alt_risk_free_rate=self.df_spy[['Risk Free Rate']])\n",
    "                df_upro = self.underlying_to_letf(self.df_spy, s=1.1, E=0.0091, beta=3,\n",
    "                                                  spread=0.005, alt_risk_free_rate=self.df_spy[['Risk Free Rate']])\n",
    "                risk_free_return = pd.DataFrame()\n",
    "                risk_free_return['Close'] = (self.df_spy['Risk Free Rate'] / 100 / 252 + 1).cumprod()\n",
    "                return {'SSO': df_sso, 'UPRO': df_upro, 'Risk Free Return': risk_free_return}\n",
    "        elif self.method in ['1.5x']:\n",
    "            if 'Risk Free Rate' in self.df_spy.columns:\n",
    "                df_15x = self.underlying_to_letf(self.df_spy, s=1.1, E=0.003, beta=1.5,\n",
    "                                                 spread=0.005, alt_risk_free_rate=self.df_spy[['Risk Free Rate']])\n",
    "                df_sso = self.underlying_to_letf(self.df_spy, s=1.1, E=0.0088, beta=2, \n",
    "                                                 spread=0.005, alt_risk_free_rate=self.df_spy[['Risk Free Rate']])\n",
    "                df_upro = self.underlying_to_letf(self.df_spy, s=1.1, E=0.0091, beta=3,\n",
    "                                                  spread=0.005, alt_risk_free_rate=self.df_spy[['Risk Free Rate']])\n",
    "                risk_free_return = pd.DataFrame()\n",
    "                risk_free_return['Close'] = (self.df_spy['Risk Free Rate'] / 100 / 252 + 1).cumprod()\n",
    "                return {'SSO': df_sso, 'UPRO': df_upro, 'Risk Free Return': risk_free_return, '1.5x': df_15x}\n",
    "        elif self.method in ['hfea']:\n",
    "            precalculated_dfs['TMF'] = self.load_tmf()\n",
    "        elif self.method in ['kmlm']:\n",
    "            precalculated_dfs['ZROZ'] = self.load_zroz()\n",
    "            precalculated_dfs['TMF'] = self.load_tmf()\n",
    "            precalculated_dfs['KMLM'] = self.load_kmlm()\n",
    "        elif self.method in ['static kmlm']:\n",
    "            precalculated_dfs['ZROZ'] = self.load_zroz()\n",
    "            precalculated_dfs['TMF'] = self.load_tmf()\n",
    "            precalculated_dfs['KMLM'] = self.load_kmlm()\n",
    "        else:\n",
    "            raise Exception('Invalid method provided to load precalculated dfs. ' +\n",
    "                            'Did you forget to add the method to TickerLoader?')\n",
    "        \n",
    "        return precalculated_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345e56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:37.981311Z",
     "start_time": "2023-09-02T01:15:37.905060Z"
    },
    "code_folding": [
     0,
     2,
     26,
     37,
     50,
     85,
     91,
     131,
     299,
     308,
     318,
     328,
     337,
     347,
     357,
     363,
     369,
     404,
     418,
     425,
     429,
     434,
     439,
     444,
     461,
     561,
     568
    ]
   },
   "outputs": [],
   "source": [
    "class Backtest:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 method,\n",
    "                 method_name,\n",
    "                 df_500,\n",
    "                 precalculated_dfs,\n",
    "                 monthly_calculation,\n",
    "                 rebalance,\n",
    "                 transaction_cost,\n",
    "                 start_date,\n",
    "                 plot_width,\n",
    "                 plot_height,\n",
    "                 line_width):\n",
    "        self.method = method\n",
    "        self.method_name = method_name\n",
    "        self.df_500 = df_500\n",
    "        self.precalculated_dfs = precalculated_dfs\n",
    "        self.monthly_calculation = monthly_calculation\n",
    "        self.rebalance = rebalance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.start_date = start_date\n",
    "        self.plot_width = plot_width\n",
    "        self.plot_height = plot_height\n",
    "        self.line_width = line_width\n",
    "\n",
    "    def simulate(self):\n",
    "        self.dates, self.port_vals, self.assets, self.port_vals_SPY, self.current_allocation = \\\n",
    "        self.simulate_backtest(df_500 = self.df_500,\n",
    "                               method = self.method,\n",
    "                               start_date = self.start_date,\n",
    "                               precalculated_dfs = self.precalculated_dfs,\n",
    "                               monthly_calculation = self.monthly_calculation,\n",
    "                               rebalance = self.rebalance,\n",
    "                               transaction_cost = self.transaction_cost)\n",
    "        return self.dates, self.port_vals, self.assets, self.port_vals_SPY, self.current_allocation\n",
    "        \n",
    "    def plot(self, dates, port_vals, assets, port_vals_SPY):\n",
    "        self.plot_value(dates,\n",
    "                        port_vals, \n",
    "                        assets,\n",
    "                        port_vals_SPY,\n",
    "                        self.method, \n",
    "                        self.method_name, \n",
    "                        self.plot_width,\n",
    "                        self.plot_height,\n",
    "                        self.line_width)\n",
    "        \n",
    "    # Credits to Leminspector for this function\n",
    "    @staticmethod\n",
    "    def underlying_to_letf(underlying,             # Underlying ETF\n",
    "                           s,                      # Swap exposure (usually around 1.1)\n",
    "                           E,                      # Expense ratio\n",
    "                           beta,                   # Leverage\n",
    "                           start=1,               # Starting price of LETF stock\n",
    "                           spread=0.004,           # Spread\n",
    "                           alt_risk_free_rate=None # Alternate risk free rate to use\n",
    "                           ):                      # Returns pandas Series object of LETF daily prices\n",
    "        daily_prices = underlying['Close']\n",
    "        daily_change = daily_prices.pct_change().dropna().rename('Daily Change')\n",
    "        \n",
    "        I = 0\n",
    "        if alt_risk_free_rate is None:  # Earliest date for FFR\n",
    "            I = web.DataReader('DFF', 'fred', daily_prices.index[0])/100 + spread  # LIBOR + spread\n",
    "            if I.index[-1] < daily_prices.index[-1]:\n",
    "                I.loc[daily_prices.index[-1]] = I['DFF'][-1]\n",
    "        else:\n",
    "            I = alt_risk_free_rate / 100 + spread\n",
    "            \n",
    "        total_costs = (s * (beta - 1) * I + E) / 252\n",
    "        \n",
    "        df = total_costs.join(daily_change, how='inner')\n",
    "        \n",
    "        \n",
    "        df['LETF Change'] = df['Daily Change'] * beta - df.iloc[:, 0]\n",
    "        df['LETF Price'] = start * (1 + df['LETF Change']).cumprod()\n",
    "\n",
    "        new_df = df['LETF Price'].to_frame(name='Close')\n",
    "\n",
    "        new_df.loc[underlying['Close'].dropna().index[0], 'Close'] = start\n",
    "        new_df = new_df.reindex(np.roll(new_df.index, shift=1))\n",
    "\n",
    "        return new_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def closest_row(df, date, column, method):\n",
    "        if column == 'index':\n",
    "            return df.index.get_indexer([date], method=method)[0]\n",
    "        else:\n",
    "            return df.iloc[df.index.get_indexer([date], method=method)[0], df.columns.get_indexer([column])[0]]\n",
    "\n",
    "    def calculate_statistics(self, dates, portfolio_values):\n",
    "        t = (dates[-1] - dates[0]).total_seconds() / 31536000  # Get years between dates\n",
    "        cagr = (((portfolio_values[-1]/portfolio_values[0])**(1/t)) - 1) * 100\n",
    "\n",
    "        df = pd.DataFrame(portfolio_values, columns=['Close'])\n",
    "        daily_returns = df.pct_change()\n",
    "        log_returns = np.log(1 + daily_returns)\n",
    "\n",
    "        tbill_sub = self.precalculated_dfs['Risk Free Return'].loc[(self.precalculated_dfs['Risk Free Return'].index \\\n",
    "                                                                    >= dates[0]) \\\n",
    "                                                                   & (self.precalculated_dfs['Risk Free Return'].index \\\n",
    "                                                                                    <= dates[-1])].copy()\n",
    "        tbill_sub['Pct Return'] = tbill_sub['Close'].pct_change()\n",
    "        log_risk_free = np.log(1 + tbill_sub['Pct Return'])\n",
    "\n",
    "        sharpe = ((log_returns.mean()[0] - log_risk_free.mean())/log_returns.std()[0])*(252**0.5)\n",
    "        if sharpe == 0:\n",
    "            # Calcualting for risk free rate\n",
    "            return portfolio_values[-1], cagr, log_returns.std()[0], 0, 0, 0, 0, 0\n",
    "        std_neg = log_returns[log_returns<0].std()\n",
    "        sortino = ((log_returns.mean()[0] - log_risk_free.mean())/std_neg[0])*(252**0.5)\n",
    "\n",
    "        max_drawdown = (df / df.cummax() - 1.0).min()[0] * 100\n",
    "\n",
    "        sum_square = 0\n",
    "        max_value = 0\n",
    "        for idx, value in enumerate(portfolio_values):\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "            else:\n",
    "                sum_square += (100 * ((value / max_value)-1)) ** 2\n",
    "        ulcer_index = (sum_square / len(portfolio_values)) ** 0.5\n",
    "\n",
    "        cagr_tbill = (((self.closest_row(tbill_sub, dates[-1], 'Close', 'nearest') / \\\n",
    "                        self.closest_row(tbill_sub, dates[0], 'Close', 'nearest'))**(1/t)) - 1) * 100\n",
    "        upi = (cagr-cagr_tbill)/ulcer_index\n",
    "\n",
    "        std_non_log = df['Close'].pct_change().std() * (252 ** 0.5)\n",
    "        return portfolio_values[-1], cagr, std_non_log, sharpe, sortino, max_drawdown, ulcer_index, upi\n",
    "    \n",
    "    def simulate_backtest(self, \n",
    "                          df_500,\n",
    "                          method,\n",
    "                          start_date, \n",
    "                          precalculated_dfs, \n",
    "                          monthly_calculation, \n",
    "                          rebalance, \n",
    "                          transaction_cost):\n",
    "        assets = []\n",
    "        current_allocation = []\n",
    "        previous_allocation = []\n",
    "        current_val = 1\n",
    "        current_val_SPY = 1\n",
    "        close = 'Close'\n",
    "        transaction_was_made = False\n",
    "\n",
    "        limiting_asset = df_500\n",
    "        for df in precalculated_dfs.values():\n",
    "            if df.dropna().index[0] > limiting_asset.dropna().index[0]:\n",
    "                limiting_asset = df\n",
    "        for key, df in precalculated_dfs.items():\n",
    "            precalculated_dfs[key] = df.loc[df.index >= limiting_asset.dropna().index[0]]\n",
    "        df_500 = df_500.loc[df_500.index >= limiting_asset.dropna().index[0]]\n",
    "\n",
    "        limiting_asset_end = df_500\n",
    "        for df in precalculated_dfs.values():\n",
    "            if df.dropna().index[-1] < limiting_asset_end.dropna().index[-1]:\n",
    "                limiting_asset_end = df\n",
    "        for key, df in precalculated_dfs.items():\n",
    "            precalculated_dfs[key] = df.loc[df.index <= limiting_asset_end.dropna().index[-1]]\n",
    "        df_500 = df_500.loc[df_500.index <= limiting_asset_end.dropna().index[-1]]\n",
    "        \n",
    "        for df in precalculated_dfs.values():\n",
    "            if len(df) != len(df_500):\n",
    "                raise Exception('Precalculated dfs must be over the same time period as the S&P 500 data')\n",
    "        \n",
    "        if monthly_calculation or rebalance == 'monthly':\n",
    "            df_500_monthly = df_500.loc[df_500.groupby(df_500.index.to_period('M')) \\\n",
    "                                        .apply(lambda x: find_middle_month(x.index))][:-1]\n",
    "            \n",
    "        if start_date:\n",
    "            start_time = self.start_date\n",
    "        else:\n",
    "            start_time = df_500.index[0]\n",
    "            \n",
    "        start_index = df_500.index.get_indexer([start_time], method='backfill')[0]\n",
    "        previous_date = df_500.index[start_index]\n",
    "\n",
    "        dates = [df_500.index[start_index]]\n",
    "        port_vals = [current_val]\n",
    "        port_vals_SPY = [current_val_SPY]\n",
    "        \n",
    "        np_signals = df_500['State'].to_numpy()\n",
    "\n",
    "        np_spy = df_500[close].to_numpy()\n",
    "        precalculated_df_values = {'SPY': np_spy}\n",
    "        for key, value in precalculated_dfs.items():\n",
    "            precalculated_df_values[key] = value[close].to_numpy()\n",
    "        \n",
    "        allocation_dict = {}\n",
    "        dynamic_method = False\n",
    "        if method in ['dynamic']:\n",
    "            dynamic_method = True\n",
    "            current_allocation = self.get_dynamic_allocation(df_500, \n",
    "                                                             df_500.index[0],\n",
    "                                                             df_500['Close'][0],\n",
    "                                                             np_signals[0],\n",
    "                                                             None,\n",
    "                                                             precalculated_dfs)\n",
    "        else:\n",
    "            allocation_dict = self.state_to_allocation_dict(df_500, precalculated_dfs)\n",
    "            current_allocation = allocation_dict[np_signals[0]]\n",
    "        \n",
    "        for idx, (date, _) in enumerate(df_500[start_index+1:].iterrows()):\n",
    "                            \n",
    "            new_asset_vals = []\n",
    "\n",
    "            for curr_asset in current_allocation[1:]:\n",
    "                np_close_values = precalculated_df_values[curr_asset[0]]\n",
    "                asset_return = np_close_values[idx + start_index + 1] / np_close_values[idx + start_index]\n",
    "                curr_asset_val = current_val * curr_asset[1]\n",
    "                \n",
    "                asset_transaction_cost = 0\n",
    "                if transaction_was_made:\n",
    "                    for prev_asset in previous_allocation[1:]:\n",
    "                        if curr_asset[0] == prev_asset[0]:\n",
    "                            percent_bought = curr_asset[1] - prev_asset[1]\n",
    "                            if percent_bought > 0:\n",
    "                                asset_transaction_cost = curr_asset_val * percent_bought * transaction_cost\n",
    "                            \n",
    "                new_asset_vals.append(curr_asset_val * asset_return - asset_transaction_cost)\n",
    "\n",
    "            current_val = sum(new_asset_vals)\n",
    "\n",
    "            # So we don't have to create a deep copy of allocation_dict\n",
    "            new_allocation = [current_allocation[0]]\n",
    "            for i, allocation in enumerate(current_allocation[1:]):\n",
    "                new_asset = [allocation[0], new_asset_vals[i] / current_val, allocation[2]]\n",
    "                new_allocation.append(new_asset)\n",
    "\n",
    "            current_allocation = new_allocation\n",
    "            \n",
    "            current_val_SPY *= np_spy[idx + start_index + 1] / np_spy[idx + start_index]\n",
    "            \n",
    "            port_vals_SPY.append(current_val_SPY)\n",
    "            port_vals.append(current_val)\n",
    "            dates.append(date)\n",
    "            assets.append(current_allocation[0])\n",
    "\n",
    "            transaction_was_made = False\n",
    "            previous_allocation = current_allocation\n",
    "            \n",
    "            if monthly_calculation and date in df_500_monthly.index:\n",
    "                if dynamic_method:\n",
    "                    new_allocation = self.get_dynamic_allocation(df_500, \n",
    "                                                                 date,\n",
    "                                                                 np_spy[idx + start_index + 1],\n",
    "                                                                 np_signals[idx + start_index + 1],\n",
    "                                                                 current_allocation,\n",
    "                                                                 precalculated_dfs)\n",
    "                else:\n",
    "                    new_allocation = allocation_dict[np_signals[idx + start_index + 1]]\n",
    "                    \n",
    "                if rebalance == 'never':\n",
    "                    for i in range(1, len(current_allocation)):\n",
    "                        if current_allocation[i][0] != new_allocation[i][0] \\\n",
    "                        or current_allocation[i][1] != new_allocation[i][1]:\n",
    "                            transaction_was_made = True\n",
    "                            current_allocation = new_allocation\n",
    "                            break\n",
    "                else:\n",
    "                    current_allocation = new_allocation       \n",
    "            else:\n",
    "                if dynamic_method:\n",
    "                    new_allocation = self.get_dynamic_allocation(df_500, \n",
    "                                                                 date,\n",
    "                                                                 np_spy[idx + start_index + 1],\n",
    "                                                                 np_signals[idx + start_index + 1],\n",
    "                                                                 current_allocation,\n",
    "                                                                 precalculated_dfs)\n",
    "                else:\n",
    "                    new_allocation = allocation_dict[np_signals[idx + start_index + 1]]\n",
    "                if rebalance == 'never':\n",
    "                    for i in range(len(current_allocation)):\n",
    "                        if current_allocation[i][0] != new_allocation[i][0]:\n",
    "                            transaction_was_made = True\n",
    "                            current_allocation = new_allocation\n",
    "                            break\n",
    "                elif rebalance == 'monthly':\n",
    "                    if date in df_500_monthly.index:\n",
    "                        transaction_was_made = True\n",
    "                        current_allocation = new_allocation\n",
    "                    else:\n",
    "                        for i in range(len(current_allocation)):\n",
    "                            if current_allocation[i][0] != new_allocation[i][0]:\n",
    "                                transaction_was_made = True\n",
    "                                current_allocation = new_allocation\n",
    "                                break\n",
    "                elif rebalance == 'daily':\n",
    "                    transaction_was_made = True\n",
    "                    current_allocation = new_allocation\n",
    "                else:\n",
    "                    print('This rebalancing frequency is not currently supported.')\n",
    "                    \n",
    "            previous_date = date\n",
    "    \n",
    "        return dates, port_vals, assets, port_vals_SPY, current_allocation\n",
    "\n",
    "    def state_to_allocation_dict(self, df_500, precalculated_dfs):\n",
    "        allocation_dict = {}\n",
    "        \n",
    "        allocation_risk_mid = []\n",
    "        if USE_SSO:\n",
    "            allocation_risk_mid = ['SSO', ['SSO', 1, precalculated_dfs['SSO']]]\n",
    "        else:\n",
    "            allocation_risk_mid = ['SSO', ['UPRO', 0.5, precalculated_dfs['UPRO']], ['SPY', 0.5, df_500]]\n",
    "            \n",
    "        if self.method == 'rfr instead of zroz and spy':\n",
    "            allocation_dict['Risk On'] = ['UPRO', \n",
    "                                          ['UPRO', 1, precalculated_dfs['UPRO']]]\n",
    "            allocation_dict['Risk Mid'] = allocation_risk_mid\n",
    "            allocation_dict['Risk Alt'] = ['Mixed', \n",
    "                                           ['UPRO', 0.25, precalculated_dfs['UPRO']], \n",
    "                                           ['Risk Free Return', 0.75, precalculated_dfs['Risk Free Return']]]\n",
    "            allocation_dict['Risk Off'] = ['Risk Free Return', \n",
    "                                            ['Risk Free Return', 1, precalculated_dfs['Risk Free Return']]]\n",
    "            \n",
    "        elif self.method == 'standard':\n",
    "            allocation_dict['Risk On'] = ['UPRO', \n",
    "                                          ['UPRO', 1, precalculated_dfs['UPRO']]]\n",
    "            allocation_dict['Risk Mid'] = allocation_risk_mid\n",
    "            allocation_dict['Risk Alt'] = ['Mixed', \n",
    "                                           ['UPRO', 0.25, precalculated_dfs['UPRO']], \n",
    "                                           ['ZROZ', 0.75, precalculated_dfs['ZROZ']]]\n",
    "            allocation_dict['Risk Off'] = ['SPY', \n",
    "                                            ['SPY', 1, df_500]]\n",
    "            \n",
    "        elif self.method == 'no risk alt':\n",
    "            allocation_dict['Risk On'] = ['UPRO', \n",
    "                                          ['UPRO', 1, precalculated_dfs['UPRO']]]\n",
    "            allocation_dict['Risk Mid'] = allocation_risk_mid\n",
    "            allocation_dict['Risk Alt'] = ['SPY', \n",
    "                                            ['SPY', 1, df_500]]\n",
    "            allocation_dict['Risk Off'] = ['SPY', \n",
    "                                            ['SPY', 1, df_500]]\n",
    "            \n",
    "        elif self.method == 'rfr instead of zroz':\n",
    "            allocation_dict['Risk On'] = ['UPRO', \n",
    "                                          ['UPRO', 1, precalculated_dfs['UPRO']]]\n",
    "            allocation_dict['Risk Mid'] = allocation_risk_mid\n",
    "            allocation_dict['Risk Alt'] = ['Mixed', \n",
    "                                           ['UPRO', 0.25, precalculated_dfs['UPRO']], \n",
    "                                           ['Risk Free Return', 0.75, precalculated_dfs['Risk Free Return']]]\n",
    "            allocation_dict['Risk Off'] = ['SPY', \n",
    "                                            ['SPY', 1, df_500]]\n",
    "            \n",
    "        elif self.method == 'rfr instead of spy':\n",
    "            allocation_dict['Risk On'] = ['UPRO', \n",
    "                                          ['UPRO', 1, precalculated_dfs['UPRO']]]\n",
    "            allocation_dict['Risk Mid'] = allocation_risk_mid\n",
    "            allocation_dict['Risk Alt'] = ['Mixed', \n",
    "                                           ['UPRO', 0.25, precalculated_dfs['UPRO']], \n",
    "                                           ['ZROZ', 0.75, precalculated_dfs['ZROZ']]]\n",
    "            allocation_dict['Risk Off'] = ['Risk Free Return', \n",
    "                                            ['Risk Free Return', 1, precalculated_dfs['Risk Free Return']]]\n",
    "            \n",
    "        elif self.method == 'hfea':\n",
    "            for state in ['Risk On', 'Risk Mid', 'Risk Alt', 'Risk Off']:\n",
    "                allocation_dict[state] = ['Mixed', \n",
    "                               ['UPRO', 0.55, precalculated_dfs['UPRO']], \n",
    "                               ['TMF', 0.45, precalculated_dfs['TMF']]]\n",
    "                \n",
    "        elif self.method == 'upro zroz':\n",
    "            for state in ['Risk On', 'Risk Mid', 'Risk Alt', 'Risk Off']:\n",
    "                allocation_dict[state] = ['Mixed', \n",
    "                                          ['UPRO', 0.40, precalculated_dfs['UPRO']],\n",
    "                                          ['ZROZ', 0.60, precalculated_dfs['ZROZ']]]\n",
    "                \n",
    "        elif self.method == 'kmlm':\n",
    "#             allocation_dict['Risk On'] = ['40/30/30',\n",
    "#                                           ['UPRO', 0.4, precalculated_dfs['UPRO']],\n",
    "#                                           ['ZROZ', 0.3, precalculated_dfs['ZROZ']],\n",
    "#                                           ['KMLM', 0.3, precalculated_dfs['KMLM']]]\n",
    "#             allocation_dict['Risk Mid'] = ['30/35/35',\n",
    "#                                           ['SSO', 0.3, precalculated_dfs['SSO']],\n",
    "#                                           ['ZROZ', 0.35, precalculated_dfs['ZROZ']],\n",
    "#                                           ['KMLM', 0.35, precalculated_dfs['KMLM']]]\n",
    "#             allocation_dict['Risk Off'] = ['10/30/60',\n",
    "#                                           ['UPRO', 0.1, df_500],\n",
    "#                                           ['Risk Free Return', 0.3, precalculated_dfs['Risk Free Return']],\n",
    "#                                           ['KMLM', 0.6, precalculated_dfs['KMLM']]]\n",
    "#             allocation_dict['Risk Alt'] = ['10/60/30',\n",
    "#                                           ['UPRO', 0.1, df_500],\n",
    "#                                           ['ZROZ', 0.6, precalculated_dfs['ZROZ']],\n",
    "#                                           ['KMLM', 0.3, precalculated_dfs['KMLM']]]\n",
    "            \n",
    "            allocation_dict['Risk On'] = ['40/30/30',\n",
    "                                          ['UPRO', 0.4, precalculated_dfs['UPRO']],\n",
    "                                          ['ZROZ', 0.3, precalculated_dfs['ZROZ']],\n",
    "                                          ['KMLM', 0.3, precalculated_dfs['KMLM']]]\n",
    "            allocation_dict['Risk Mid'] = ['30/35/35',\n",
    "                                          ['SSO', 0.4, precalculated_dfs['SSO']],\n",
    "                                          ['ZROZ', 0.3, precalculated_dfs['ZROZ']],\n",
    "                                          ['KMLM', 0.3, precalculated_dfs['KMLM']]]\n",
    "            allocation_dict['Risk Off'] = ['10/30/60',\n",
    "                                          ['SPY', 0.4, df_500],\n",
    "                                          ['Risk Free Return', 0.3, precalculated_dfs['Risk Free Return']],\n",
    "                                          ['KMLM', 0.3, precalculated_dfs['KMLM']]]\n",
    "            allocation_dict['Risk Alt'] = ['10/60/30',\n",
    "                                          ['SPY', 0.4, df_500],\n",
    "                                          ['ZROZ', 0.3, precalculated_dfs['ZROZ']],\n",
    "                                          ['KMLM', 0.3, precalculated_dfs['KMLM']]]\n",
    "            \n",
    "        elif self.method == 'simple':\n",
    "            allocation_dict['Risk On'] = ['SPY',\n",
    "                                          ['SPY', 1, df_500]]\n",
    "                                          \n",
    "            allocation_dict['Risk Mid'] = ['Mixed',\n",
    "                                           ['Risk Free Return', 0.5, precalculated_dfs['Risk Free Return']],\n",
    "                                           ['SPY', 0.5, df_500]]\n",
    "                                           \n",
    "            allocation_dict['Risk Off'] = ['Risk Free Return',\n",
    "                                           ['Risk Free Return', 1, precalculated_dfs['Risk Free Return']]]\n",
    "                                           \n",
    "            allocation_dict['Risk Alt'] = ['Risk Free Return',\n",
    "                                           ['Risk Free Return', 1, precalculated_dfs['Risk Free Return']]]\n",
    "\n",
    "        elif self.method == 'static kmlm':\n",
    "            for state in ['Risk On', 'Risk Mid', 'Risk Alt', 'Risk Off']:\n",
    "                allocation_dict[state] = ['Mixed',\n",
    "                                          ['UPRO', 0.4, precalculated_dfs['UPRO']],\n",
    "                                          ['ZROZ', 0.3, precalculated_dfs['ZROZ']],\n",
    "                                          ['KMLM', 0.3, precalculated_dfs['KMLM']]]\n",
    "                \n",
    "        elif self.method == 'sso':\n",
    "            for state in ['Risk On', 'Risk Mid', 'Risk Alt', 'Risk Off']:\n",
    "                allocation_dict[state] = allocation_risk_mid\n",
    "                \n",
    "        elif self.method == 'upro':\n",
    "            for state in ['Risk On', 'Risk Mid', 'Risk Alt', 'Risk Off']:\n",
    "                allocation_dict[state] = ['UPRO',\n",
    "                                         ['UPRO', 1, precalculated_dfs['UPRO']]]\n",
    "                \n",
    "        elif self.method == '1.5x':\n",
    "            for state in ['Risk On', 'Risk Mid', 'Risk Alt', 'Risk Off']:\n",
    "                allocation_dict[state] = ['1.5x',\n",
    "                                         ['1.5x', 1, precalculated_dfs['1.5x']]]           \n",
    "                \n",
    "        else:\n",
    "            raise Exception('Invalid method')\n",
    "            \n",
    "        return allocation_dict\n",
    "    \n",
    "    def get_dynamic_allocation(self, df_500, date, current_close, signal, current_allocation, precalculated_dfs):\n",
    "        if USE_SSO:\n",
    "            allocation_risk_mid = ['SSO', ['SSO', 1, precalculated_dfs['SSO']]]\n",
    "        else:\n",
    "            allocation_risk_mid = ['SSO', ['UPRO', 0.5, precalculated_dfs['UPRO']], ['SPY', 0.5, df_500]]\n",
    "            \n",
    "        if self.method == 'dynamic':\n",
    "            if state == 'Risk On':\n",
    "                return ['UPRO', ['UPRO', 1, precalculated_dfs['UPRO']]]\n",
    "            elif state == 'Risk Mid':\n",
    "                return allocation_risk_mid\n",
    "            elif state == 'Risk Alt':\n",
    "                return ['Mixed', ['UPRO', 0.25, precalculated_dfs['UPRO']], \n",
    "                        ['Risk Free Return', 0.75, precalculated_dfs['Risk Free Return']]]\n",
    "            else:\n",
    "                return ['Risk Free Return', ['Risk Free Return', 1, precalculated_dfs['Risk Free Return']]]\n",
    "    \n",
    "    def plot_value(self, \n",
    "                   dates, \n",
    "                   port_vals, \n",
    "                   assets, \n",
    "                   port_vals_SPY, \n",
    "                   method, \n",
    "                   method_name,\n",
    "                   plot_width,\n",
    "                   plot_height,\n",
    "                   line_width):\n",
    "        df_500_subset = self.df_500.loc[(self.df_500.index >= dates[0]) & (self.df_500.index <= dates[-1])]\n",
    "\n",
    "        np_dates = np.array(dates, dtype=\"datetime64[D]\")\n",
    "        y = np.array(port_vals)\n",
    "\n",
    "        color_list = []\n",
    "        for asset in assets:\n",
    "            if asset == 'SPY':\n",
    "                color_list.append('red')\n",
    "            elif asset == 'SSO':\n",
    "                color_list.append('green')\n",
    "            elif asset == 'UPRO':\n",
    "                color_list.append('blue')\n",
    "            elif asset == 'Risk Free Return':\n",
    "                color_list.append('gray')\n",
    "            elif asset == 'Mixed':\n",
    "                color_list.append('purple')\n",
    "            elif asset == '40/30/30':\n",
    "                color_list.append('red')\n",
    "            elif asset == '30/35/35':\n",
    "                color_list.append('green')\n",
    "            elif asset == '10/30/60':\n",
    "                color_list.append('blue')\n",
    "            elif asset == '10/60/30':\n",
    "                color_list.append('purple')\n",
    "            elif asset == '1.5x':\n",
    "                color_list.append('purple')\n",
    "                \n",
    "        inxval = mdates.date2num(np_dates)\n",
    "        points = np.array([inxval, y]).T.reshape(-1,1,2)\n",
    "        segments = np.concatenate([points[:-1],points[1:]], axis=1)\n",
    "\n",
    "        lc = LineCollection(segments, colors=color_list, linewidths=line_width)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
    "        ax.add_collection(lc)\n",
    "        fig.autofmt_xdate()\n",
    "        fig.patch.set_facecolor('white')\n",
    "\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_ha('center')\n",
    "            label.set_rotation(0)\n",
    "\n",
    "        plt.yscale('log')\n",
    "        plt.xlim(dates[0], dates[-1])\n",
    "        ax.autoscale()\n",
    "\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        line1 = Line2D([0], [0], label='1x Leveraged S&P 500', color='r')\n",
    "        line2 = Line2D([0], [0], label='2x Leveraged S&P 500', color='green')\n",
    "        line3 = Line2D([0], [0], label='3x Leveraged S&P 500', color='blue')\n",
    "        line4 = Line2D([0], [0], label='Mixed', color='purple')\n",
    "        line5 = Line2D([0], [0], label='Risk Free Return', color='gray')\n",
    "        handles.extend([line1, line2, line3, line4, line5])\n",
    "\n",
    "        plt.legend(handles=handles, loc = 'upper left')\n",
    "\n",
    "        plt.xlabel('Date', fontsize=14)\n",
    "        plt.ylabel('Portfolio Value (Starting at $1)', fontsize=14)\n",
    "        plt.title(f'{method_name}, ' + dates[0].strftime('%b %Y') + ' - ' + \\\n",
    "                  dates[-1].strftime('%b %Y'), fontsize=14)\n",
    "        \n",
    "        def get_comparison_port_vals(ticker):\n",
    "            df_internal = self.precalculated_dfs[ticker]\n",
    "            df_internal_sub = df_internal.loc[(df_internal.index >= dates[0]) & (df_internal.index <= dates[-1])]\n",
    "            port_vals_internal = list((df_internal_sub['Close'].pct_change()+1).cumprod().dropna())\n",
    "            port_vals_internal.insert(0, 1)\n",
    "            return np.array(port_vals_internal)\n",
    "\n",
    "        plt.plot(np_dates, np.array(port_vals_SPY), color='r', alpha=0.2)\n",
    "        plt.plot(np_dates, get_comparison_port_vals('SSO'), color='green', alpha=0.2)\n",
    "        plt.plot(np_dates, get_comparison_port_vals('UPRO'), color='blue', alpha=0.2)\n",
    "        plt.plot(np_dates, get_comparison_port_vals('Risk Free Return'), color='gray', alpha=0.2)\n",
    "        \n",
    "        df_stats = pd.DataFrame(columns=['Strategy', 'Ending Value', 'CAGR', 'Standard Deviation', 'Sharpe', \n",
    "                                         'Sortino', 'Max Drawdown', 'Ulcer Index', 'Ulcer Performance Index'])\n",
    "        df_stats.loc[len(df_stats)] = [f'{method_name}'] + \\\n",
    "            [x for x in self.calculate_statistics(dates, port_vals)]\n",
    "        df_stats.loc[len(df_stats)] = ['S&P 500 Large'] + \\\n",
    "            [x for x in self.calculate_statistics(dates, port_vals_SPY)]\n",
    "        df_stats.loc[len(df_stats)] = ['2x S&P 500 Large'] + \\\n",
    "            [x for x in self.calculate_statistics(dates, get_comparison_port_vals('SSO'))]\n",
    "        df_stats.loc[len(df_stats)] = ['3x S&P 500 Large'] + \\\n",
    "            [x for x in self.calculate_statistics(dates, get_comparison_port_vals('UPRO'))]\n",
    "        df_stats.loc[len(df_stats)] = ['Risk Free Return'] + \\\n",
    "            [x for x in self.calculate_statistics(dates, get_comparison_port_vals('Risk Free Return'))]\n",
    "\n",
    "        plt.show()\n",
    "        display(df_stats)\n",
    "        \n",
    "    def get_strategy_results(self):\n",
    "        zipped = list(zip(self.dates, self.port_vals, self.assets))\n",
    "        df = pd.DataFrame(zipped, columns=['Date', 'Close', 'Asset Held'])\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.set_index('Date')\n",
    "        return df\n",
    "    \n",
    "    def get_spy_results(self):\n",
    "        zipped = list(zip(self.dates, self.port_vals_SPY))\n",
    "        df_SPY = pd.DataFrame(zipped, columns=['Date', 'Close'])\n",
    "        df_SPY['Date'] = pd.to_datetime(df_SPY['Date'])\n",
    "        df_SPY = df_SPY.set_index('Date')\n",
    "        return df_SPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb6684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:38.121636Z",
     "start_time": "2023-09-02T01:15:38.112635Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def drawdown_plot(df_results, df_spy, plot_width, plot_height, method_name):\n",
    "    fig = plt.figure(figsize = (plot_width, plot_height))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Drawdown Percentage', fontsize=14)\n",
    "    plt.title(f'{method_name} Drawdown Chart, ' \\\n",
    "              + df_results.index[0].strftime('%b %Y') + ' - ' \\\n",
    "              + df_results.index[-1].strftime('%b %Y'), fontsize=14)\n",
    "\n",
    "    rolling_max = df_results['Close'].cummax()\n",
    "    drawdown_result = (df_results['Close'] - rolling_max) / rolling_max\n",
    "    drawdown_result *= 100\n",
    "\n",
    "    rolling_max = df_spy['Close'].cummax()\n",
    "    drawdown_spy = (df_spy['Close'] - rolling_max) / rolling_max\n",
    "    drawdown_spy *= 100\n",
    "\n",
    "    plt.plot(drawdown_result, color='navy', label='Strategy')\n",
    "\n",
    "    plt.fill_between(drawdown_spy.index, 0, drawdown_spy, alpha=0.3, label='S&P 500')\n",
    "\n",
    "    plt.legend(loc = 'lower left')\n",
    "    plt.show()\n",
    "    \n",
    "    return drawdown_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29cf274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:38.402157Z",
     "start_time": "2023-09-02T01:15:38.394589Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_rolling_returns(df_results, df_spy, years, plot_width, plot_height, method_name):\n",
    "    x = rolling_window(df_results['Close'].to_numpy(), 252 * years)\n",
    "    rolling_returns = (((x[:, -1] / x[:, 0]) ** (1 / years)) - 1) * 100\n",
    "    rolling_returns = np.insert(rolling_returns, 0, [None] * (years * 252 - 1))\n",
    "    \n",
    "    \n",
    "    x = rolling_window(df_spy['Close'].to_numpy(), 252 * years)\n",
    "    rolling_spy = (((x[:, -1] / x[:, 0]) ** (1 / years)) - 1) * 100\n",
    "    rolling_spy = np.insert(rolling_spy, 0, [None] * (years * 252 - 1))\n",
    "\n",
    "    fig = plt.figure(figsize = (plot_width, plot_height))\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Rolling CAGR', fontsize=14)\n",
    "    plt.title(f'{method_name} Rolling Returns ({years} years), ' \\\n",
    "              + df_results.index[0].strftime('%b %Y') + ' - ' \\\n",
    "              + df_results.index[-1].strftime('%b %Y'), fontsize=14)\n",
    "\n",
    "    plt.plot(df_results.index, rolling_returns, color='blue', linewidth=1, label=method_name)\n",
    "    plt.plot(df_spy.index, rolling_spy, color='red', alpha=0.5, linewidth=1, label='S&P 500')\n",
    "    plt.axhline(y=0, color='black', linestyle='--')\n",
    "    \n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return rolling_returns[~np.isnan(rolling_returns)], rolling_spy[~np.isnan(rolling_spy)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88198e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T02:01:28.622457Z",
     "start_time": "2023-09-02T02:01:28.606736Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def rolling_correlation(df1, df2, years, plot_width, plot_height):\n",
    "    rolling_length = 252 * years\n",
    "    \n",
    "    rolling_1 = rolling_window(df1['Close'].pct_change().dropna().to_numpy(), rolling_length)\n",
    "    rolling_2 = rolling_window(df2['Close'].pct_change().dropna().to_numpy(), rolling_length)\n",
    "    \n",
    "    results = []\n",
    "    for idx, row in enumerate(rolling_1):\n",
    "        results.append(np.corrcoef(row, rolling_2[idx])[0][1])\n",
    "        \n",
    "    dates = df1.iloc[rolling_length:].index\n",
    "    \n",
    "    fig = plt.figure(figsize = (plot_width, plot_height))\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel(f'Rolling {years} Year Correlation', fontsize=14)\n",
    "    plt.title(f'Rolling {years} Year Correlation, ' \\\n",
    "              + dates[0].strftime('%b %Y') + ' - ' \\\n",
    "              + dates[-1].strftime('%b %Y'), fontsize=14)\n",
    "\n",
    "    plt.plot(dates, results, color='blue', linewidth=1)\n",
    "    plt.axhline(y=0, color='black', linestyle='--')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a4d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:38.572220Z",
     "start_time": "2023-09-02T01:15:38.557807Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def new_cagr_vol(df, new_cagr, new_vol):\n",
    "    new_cagr /= 100 # CAGR should be a percent, vol shouldn't\n",
    "    df = df.copy()\n",
    "    dates = df.index\n",
    "    portfolio_values = list(df['Close'])\n",
    "    rplus1 = df['Close'].pct_change() + 1\n",
    "    old_cagr = (((portfolio_values[-1]/portfolio_values[0])**(252/len(df))) - 1)\n",
    "    old_vol = rplus1.std() * (252 ** 0.5)\n",
    "    \n",
    "    beta = new_vol / old_vol\n",
    "    alpha = math.log(1 + new_cagr) - (beta * math.log(1 + old_cagr)) \\\n",
    "            + (1/2) * (beta ** 2 - beta) * 252 * ((rplus1 - 1) ** 2).mean() \\\n",
    "            - (1/3) * (beta ** 3 - beta) * 252 * ((rplus1 - 1) ** 3).mean() \\\n",
    "            + (1/4) * (beta ** 4 - beta) * 252 * ((rplus1 - 1) ** 4).mean() \\\n",
    "            - (1/5) * (beta ** 5 - beta) * 252 * ((rplus1 - 1) ** 5).mean()\n",
    "    \n",
    "    alpha /= 252\n",
    "    \n",
    "    rplus1 = 1 + (beta * (rplus1 - 1) + alpha)\n",
    "    \n",
    "    df['Close'] = rplus1.cumprod()\n",
    "    df.loc[df.index[0], 'Close'] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a3fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:38.728572Z",
     "start_time": "2023-09-02T01:15:38.704859Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def simulate_random_scenarios(df, method, num_simulations, print_results, modified_cagr = None, modified_vol = None):\n",
    "    df_orig = df.copy()\n",
    "    cagrs = []\n",
    "    sharpes = []\n",
    "    upis = []\n",
    "    max_dds = []\n",
    "    \n",
    "    if modified_cagr and modified_vol:\n",
    "        df = new_cagr_vol(df, modified_cagr, modified_vol)\n",
    "    \n",
    "    vol_lookback_days = 21\n",
    "    mom_lookback_days = 252\n",
    "    short_mom_lookback_days = 126\n",
    "    tbill_lookback_days = 21\n",
    "    monthly = False\n",
    "    low_vol_cutoff = 0.14\n",
    "    high_vol_cutoff = 0.24\n",
    "    strict_boundaries = False\n",
    "    use_sma = True\n",
    "    rebalance = 'daily'\n",
    "    \n",
    "    ticker_loader = TickerLoader(df, method)\n",
    "    precalculated_dfs = ticker_loader.load_precalculated_dfs()\n",
    "    \n",
    "    if 'Risk Free Rate' in df.columns:\n",
    "        df = load_df_data(df, vol_lookback_days, mom_lookback_days, short_mom_lookback_days, tbill_lookback_days,\n",
    "                          False, use_sma, monthly, use_tbill_data = False)\n",
    "    else:\n",
    "        df = load_df_data(df, vol_lookback_days, mom_lookback_days, short_mom_lookback_days, tbill_lookback_days,\n",
    "                          False, use_sma, monthly, use_tbill_data = True)\n",
    "\n",
    "    df = calculate_signals(df, low_vol_cutoff, high_vol_cutoff, \n",
    "                           strict_boundaries, False, False, monthly)\n",
    "\n",
    "    backtest = Backtest(method = method,\n",
    "                        method_name = 'this does not matter',\n",
    "                        df_500 = df,\n",
    "                        precalculated_dfs = precalculated_dfs,\n",
    "                        monthly_calculation = monthly,\n",
    "                        rebalance = rebalance,\n",
    "                        transaction_cost = 0.001,\n",
    "                        start_date = None,\n",
    "                        plot_width = 10, \n",
    "                        plot_height = 10, \n",
    "                        line_width = 10)\n",
    "\n",
    "    dates_base, port_vals, assets, port_vals_SPY, _ = backtest.simulate()\n",
    "    \n",
    "    _, cagr_base, vol_base, sharpe_base, _, dd_base, _, upi_base = backtest.calculate_statistics(dates_base, port_vals)\n",
    "    _, cagr_spy, vol_spy, sharpe_spy, _, dd_spy, _, upi_spy = backtest.calculate_statistics(dates_base, port_vals_SPY)\n",
    "    \n",
    "    cagrs.extend([cagr_spy, cagr_base])\n",
    "    sharpes.extend([sharpe_spy, sharpe_base])\n",
    "    max_dds.extend([dd_spy, dd_base])\n",
    "    upis.extend([upi_spy, upi_base])\n",
    "    \n",
    "    if print_results:\n",
    "        print(f'\\nDates: {dates_base[0].strftime(\"%B %d, %Y\")} to {dates_base[-1].strftime(\"%B %d, %Y\")}\\n')\n",
    "        print(f'=========== S&P 500 Statistics ===========')\n",
    "        print(f'{\"CAGR\":<25} {cagr_spy:>10.3f}')\n",
    "        print(f'{\"Volatility\":<25} {vol_spy:>10.3f}')\n",
    "        print(f'{\"Sharpe\":<25} {sharpe_spy:>10.3f}')\n",
    "        print(f'{\"UPI\":<25} {upi_spy:>10.3f}')\n",
    "        print(f'{\"Max Drawdown\":<25} {dd_spy:>10.3f}\\n')\n",
    "\n",
    "        print(f'=========== Baseline Strategy ===========')\n",
    "        print(f'{\"Lower volatility cutoff\":<25} {low_vol_cutoff:>10.3f}')\n",
    "        print(f'{\"Upper volatility cutoff\":<25} {high_vol_cutoff:>10.3f}')\n",
    "        print(f'{\"Volatility lookback days\":<25} {vol_lookback_days:>10}')\n",
    "        print(f'{\"Momentum lookback days\":<25} {mom_lookback_days:>10}')\n",
    "        print(f'{\"T-bill lookback days\":<25} {tbill_lookback_days:>10}')\n",
    "        print(f'{\"Monthly calculation\":<25} {str(monthly):>10}')\n",
    "        print(f'{\"Strict boundaries\":<25} {str(strict_boundaries):>10}')\n",
    "        print(f'{\"Use SMA\":<25} {str(use_sma):>10}')\n",
    "        print(f'{\"Rebalance frequency\":<25} {rebalance:>10}')\n",
    "        print(f'           --- Results ---')\n",
    "        print(f'{\"CAGR\":<25} {cagr_base:>10.3f}')\n",
    "        print(f'{\"Volatility\":<25} {vol_base:>10.3f}')\n",
    "        print(f'{\"Sharpe\":<25} {sharpe_base:>10.3f}')\n",
    "        print(f'{\"UPI\":<25} {upi_base:>10.3f}')\n",
    "        print(f'{\"Max Drawdown\":<25} {dd_base:>10.3f}\\n')\n",
    "            \n",
    "    for i in range(num_simulations):\n",
    "        vol_lookback_days = random.randrange(10, 30)\n",
    "        mom_lookback_days = random.randrange(126, 252)\n",
    "        short_mom_lookback_days = random.randrange(63, 126)\n",
    "        tbill_lookback_days = random.randrange(10, 30)\n",
    "        monthly = random.choice([False])\n",
    "        low_vol_cutoff = random.uniform(0.1, 0.2)\n",
    "        high_vol_cutoff = low_vol_cutoff + random.uniform(0.05, 0.15)\n",
    "        strict_boundaries = random.choice([True, False])\n",
    "        use_sma = random.choice([True, False])\n",
    "        rebalance = random.choice(['daily', 'monthly', 'never'])\n",
    "\n",
    "        # Not using linear regression because it is slower\n",
    "        if 'Risk Free Rate' in df.columns:\n",
    "            df = load_df_data(df, vol_lookback_days, mom_lookback_days, short_mom_lookback_days, tbill_lookback_days,\n",
    "                              False, use_sma, monthly, use_tbill_data = False)\n",
    "        else:\n",
    "            df = load_df_data(df, vol_lookback_days, mom_lookback_days, short_mom_lookback_days, tbill_lookback_days,\n",
    "                              False, use_sma, monthly, use_tbill_data = True)\n",
    "\n",
    "        df = calculate_signals(df, low_vol_cutoff, high_vol_cutoff, \n",
    "                               strict_boundaries, False, False, monthly)\n",
    "    \n",
    "        backtest = Backtest(method = method,\n",
    "                            method_name = 'this does not matter',\n",
    "                            df_500 = df,\n",
    "                            precalculated_dfs = precalculated_dfs,\n",
    "                            monthly_calculation = monthly,\n",
    "                            rebalance = rebalance,\n",
    "                            transaction_cost = 0.001,\n",
    "                            start_date = dates_base[0],\n",
    "                            plot_width = 10, \n",
    "                            plot_height = 10, \n",
    "                            line_width = 10)\n",
    "        \n",
    "        dates, port_vals, assets, port_vals_SPY, _ = backtest.simulate()\n",
    "        _, cagr, vol, sharpe, _, max_drawdown, _, upi = backtest.calculate_statistics(dates, port_vals)\n",
    "        cagrs.append(cagr)\n",
    "        sharpes.append(sharpe)\n",
    "        upis.append(upi)\n",
    "        max_dds.append(max_drawdown)\n",
    "        \n",
    "        if print_results:\n",
    "            print(f'=========== Simulation {i + 1} ===========')\n",
    "            print(f'{\"Lower volatility cutoff\":<25} {low_vol_cutoff:>10.3f}')\n",
    "            print(f'{\"Upper volatility cutoff\":<25} {high_vol_cutoff:>10.3f}')\n",
    "            print(f'{\"Volatility lookback days\":<25} {vol_lookback_days:>10}')\n",
    "            print(f'{\"Momentum lookback days\":<25} {mom_lookback_days:>10}')\n",
    "            print(f'{\"T-bill lookback days\":<25} {tbill_lookback_days:>10}')\n",
    "            print(f'{\"Monthly calculation\":<25} {str(monthly):>10}')\n",
    "            print(f'{\"Strict boundaries\":<25} {str(strict_boundaries):>10}')\n",
    "            print(f'{\"Use SMA\":<25} {str(use_sma):>10}')\n",
    "            print(f'{\"Rebalance frequency\":<25} {rebalance:>10}')\n",
    "\n",
    "            print(f'           --- Results ---')\n",
    "            print(f'{\"CAGR\":<25} {cagr:>10.3f}')\n",
    "            print(f'{\"Volatility\":<25} {vol:>10.3f}')\n",
    "            print(f'{\"Sharpe\":<25} {sharpe:>10.3f}')\n",
    "            print(f'{\"UPI\":<25} {upi:>10.3f}')\n",
    "            print(f'{\"Max Drawdown\":<25} {max_drawdown:>10.3f}\\n')\n",
    "    \n",
    "    simulation = ['S&P 500', 'Baseline'] + list(range(num_simulations))\n",
    "    zipped = list(zip(simulation, cagrs, sharpes, max_dds, upis))\n",
    "    df_results = pd.DataFrame(zipped, columns=['Simulation', 'CAGR', 'Sharpe', 'Max Drawdown', 'UPI'])\n",
    "    df_results = df_results.set_index('Simulation')\n",
    "    \n",
    "    return df_results, dates_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c0c93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:38.866403Z",
     "start_time": "2023-09-02T01:15:38.851352Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def simulate_prompt():\n",
    "    df_results = pd.DataFrame()\n",
    "    dates = []\n",
    "\n",
    "    num_sims = int(input('Number of simulations? '))\n",
    "    if num_sims <= 0:\n",
    "        return\n",
    "    method = input('Method? ')\n",
    "    \n",
    "    print_results = input('Print individual simulation results? [Y/N] ')\n",
    "    if print_results.upper() == 'Y' or print_results.upper() == 'YES':\n",
    "        print_results = True\n",
    "    elif print_results.upper() == 'N' or print_results.upper() == 'NO':\n",
    "        print_results = False\n",
    "    else:\n",
    "        raise Exception('Invalid input')\n",
    "        \n",
    "    sp_500 = input('Use simulated extended S&P 500 data [Y] or use ticker data [N]? ')\n",
    "\n",
    "    df_sim = pd.DataFrame()\n",
    "    if sp_500.upper() == 'Y' or sp_500.upper() == 'YES':\n",
    "        df_sim = load_us_market_data(EXTENDED_DATA_PATH)\n",
    "    elif sp_500.upper() == 'N' or sp_500.upper() == 'NO':\n",
    "        ticker = input('Ticker data to use? ')\n",
    "        df_sim = load_additional_dfs(ticker.upper())\n",
    "    else:\n",
    "        raise Exception('Invalid input')\n",
    "    use_modified = input('Use modified S&P 500 data? [Y/N] ')\n",
    "    if use_modified.upper() == 'Y' or use_modified.upper() == 'YES':\n",
    "        print(\"\\nNote that the inputted CAGR will not line up with the S&P 500 statistics. This is because the CAGR given \"\n",
    "            + \"is applied over the ENTIRE time period of S&P 500 data available. \\nHowever, when the backtest is done, a chunk of the \"\n",
    "            + \"start is cut off because momentum needs to be calculated before making an initial allocation.\\n\")\n",
    "        modified_cagr = float(input('Modified S&P 500 CAGR? [Input as percentage without % symbol] '))\n",
    "        modified_vol = float(input('Modified S&P 500 Vol? [Input as decimal] '))\n",
    "        df_results, dates = simulate_random_scenarios(df_sim, method, num_sims, print_results, modified_cagr, modified_vol)\n",
    "    elif use_modified.upper() == 'N' or use_modified.upper() == 'NO':\n",
    "        df_results, dates = simulate_random_scenarios(df_sim, method, num_sims, print_results)\n",
    "    else:\n",
    "        raise Exception('Invalid input')\n",
    "    \n",
    "    plot_sim_results(df_results, dates, num_sims, 15, 10)\n",
    "    \n",
    "    return df_results, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b7548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:39.114044Z",
     "start_time": "2023-09-02T01:15:38.995830Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_sim_results(df_sim_results, dates, num_simulations, plot_width, plot_height):\n",
    "    fig, axs = plt.subplots(2, 2, figsize = (plot_width, plot_height), tight_layout = True)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    cagr_spy = df_sim_results['CAGR'].iloc[0]\n",
    "    sharpe_spy = df_sim_results['Sharpe'].iloc[0]\n",
    "    upi_spy = df_sim_results['UPI'].iloc[0]\n",
    "    dd_spy = df_sim_results['Max Drawdown'].iloc[0]\n",
    "    \n",
    "    cagr_base = df_sim_results['CAGR'].iloc[1]\n",
    "    sharpe_base = df_sim_results['Sharpe'].iloc[1]\n",
    "    upi_base = df_sim_results['UPI'].iloc[1]\n",
    "    dd_base = df_sim_results['Max Drawdown'].iloc[1]\n",
    "    \n",
    "    axs[0][0].hist(x = df_sim_results.iloc[2:]['CAGR'].values, bins = 30, color = 'blue')\n",
    "    axs[0][0].axvline(x = cagr_base, color = 'green', label = 'Base Strategy CAGR')\n",
    "    axs[0][0].axvline(x = cagr_spy, color = 'red', label = 'S&P 500 CAGR')\n",
    "    axs[0][0].title.set_text('CAGRs')\n",
    "    axs[0][0].set_xlabel('CAGR')\n",
    "    axs[0][0].set_ylabel('Count')\n",
    "    axs[0][0].legend(loc = 'upper left')\n",
    "    \n",
    "    axs[1][0].hist(x = df_sim_results.iloc[2:]['Sharpe'].values, bins = 30, color = 'blue')\n",
    "    axs[1][0].axvline(x = sharpe_base, color = 'green', label = 'Base Strategy Sharpe')\n",
    "    axs[1][0].axvline(x = sharpe_spy, color = 'red', label = 'S&P 500 Sharpe')\n",
    "    axs[1][0].title.set_text('Sharpes')\n",
    "    axs[1][0].set_xlabel('Sharpe')\n",
    "    axs[1][0].set_ylabel('Count')\n",
    "    axs[1][0].legend(loc = 'upper left')\n",
    "    \n",
    "    axs[0][1].hist(x = df_sim_results.iloc[2:]['UPI'].values, bins = 30, color = 'blue')\n",
    "    axs[0][1].axvline(x = upi_base, color = 'green', label = 'Base Strategy UPI')\n",
    "    axs[0][1].axvline(x = upi_spy, color = 'red', label = 'S&P 500 UPI')\n",
    "    axs[0][1].title.set_text('Ulcer Performance Indices')\n",
    "    axs[0][1].set_xlabel('Ulcer Performance Index')\n",
    "    axs[0][1].set_ylabel('Count')\n",
    "    axs[0][1].legend(loc = 'upper left')\n",
    "    \n",
    "    axs[1][1].hist(x = df_sim_results.iloc[2:]['Max Drawdown'].values, bins = 30, color = 'blue')\n",
    "    axs[1][1].axvline(x = dd_base, color = 'green', label = 'Base Strategy DD')\n",
    "    axs[1][1].axvline(x = dd_spy, color = 'red', label = 'S&P 500 DD')\n",
    "    axs[1][1].title.set_text('Max Drawdowns')\n",
    "    axs[1][1].set_xlabel('Max Drawdown')\n",
    "    axs[1][1].set_ylabel('Count')\n",
    "    axs[1][1].legend(loc = 'upper left')\n",
    "    \n",
    "    fig.suptitle(f'Results of {num_simulations} simulations, ' + \\\n",
    "                 f'{dates[0].strftime(\"%B %d, %Y\")} to {dates[-1].strftime(\"%B %d, %Y\")}', fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b963344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:39.160890Z",
     "start_time": "2023-09-02T01:15:39.131822Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    time1 = time.time()\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if USE_EXTENDED_DATA:\n",
    "        df = load_us_market_data(EXTENDED_DATA_PATH)\n",
    "        if USE_MODIFIED_SP_500:\n",
    "            df = new_cagr_vol(df, MODIFIED_SP_500_CAGR, MODIFIED_SP_500_VOL)\n",
    "        time2 = time.time()\n",
    "        if USE_DAILY_TBILL:\n",
    "            df = df[['Close']]\n",
    "            df = load_df_data(df, VOLATILITY_LOOKBACK_DAYS, MOMENTUM_LOOKBACK_DAYS, SHORT_MOMENTUM_LOOKBACK_DAYS, \n",
    "                              TBILL_LOOKBACK_DAYS, USE_LINEAR_REGRESSION, USE_SMA, MONTHLY_CALCULATION, \n",
    "                              use_tbill_data = True)\n",
    "        else:\n",
    "            df = load_df_data(df, VOLATILITY_LOOKBACK_DAYS, MOMENTUM_LOOKBACK_DAYS, SHORT_MOMENTUM_LOOKBACK_DAYS,\n",
    "                              TBILL_LOOKBACK_DAYS, USE_LINEAR_REGRESSION, USE_SMA, MONTHLY_CALCULATION, \n",
    "                              use_tbill_data = False)\n",
    "    else:\n",
    "        df = load_additional_dfs(SP_500_TICKER)\n",
    "        if USE_MODIFIED_SP_500:\n",
    "            df = new_cagr_vol(df, MODIFIED_SP_500_CAGR, MODIFIED_SP_500_VOL)\n",
    "        time2 = time.time()\n",
    "        df = load_df_data(df, VOLATILITY_LOOKBACK_DAYS, MOMENTUM_LOOKBACK_DAYS, SHORT_MOMENTUM_LOOKBACK_DAYS, \n",
    "                          TBILL_LOOKBACK_DAYS, USE_LINEAR_REGRESSION, USE_SMA, MONTHLY_CALCULATION, \n",
    "                          use_tbill_data = True)\n",
    "    \n",
    "    time3 = time.time()\n",
    "    df = calculate_signals(df, LOWER_VOL_CUTOFF, HIGHER_VOL_CUTOFF, \n",
    "                           USE_STRICT_BOUNDARIES, EXCLUDE_VOL, EXCLUDE_MOM, MONTHLY_CALCULATION)\n",
    "    time4 = time.time()\n",
    "    \n",
    "    if USE_MODIFIED_SP_500:\n",
    "        df = new_cagr_vol(df, MODIFIED_SP_500_CAGR, MODIFIED_SP_500_VOL)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    mpl.rcParams['figure.dpi'] = PLOT_DPI\n",
    "    mpl.rcParams['font.family'] = PLOT_FONT\n",
    "\n",
    "    plot_signal(df, PLOT_WIDTH, PLOT_HEIGHT, \n",
    "                SHOW_RISK_ON, SHOW_RISK_MID, SHOW_RISK_ALT, SHOW_RISK_OFF, \n",
    "                LOWER_VOL_CUTOFF, HIGHER_VOL_CUTOFF)\n",
    "    time5 = time.time()\n",
    "\n",
    "    ticker_loader = TickerLoader(method = METHOD, df_spy = df)\n",
    "    precalculated_dfs = ticker_loader.load_precalculated_dfs()\n",
    "    \n",
    "    backtest = Backtest(method = METHOD,\n",
    "                        method_name = METHOD_NAME,\n",
    "                        df_500 = df,\n",
    "                        precalculated_dfs = precalculated_dfs,\n",
    "                        monthly_calculation = MONTHLY_CALCULATION,\n",
    "                        rebalance = REBALANCE,\n",
    "                        transaction_cost = TRANSACTION_COST,\n",
    "                        start_date = BACKTEST_START_DATE,\n",
    "                        plot_width = PLOT_WIDTH, \n",
    "                        plot_height = PLOT_HEIGHT, \n",
    "                        line_width = BACKTEST_LINE_WIDTH)\n",
    "\n",
    "    time6 = time.time()\n",
    "    dates, port_vals, assets, port_vals_SPY, current_allocation = backtest.simulate()\n",
    "    time7 = time.time()\n",
    "\n",
    "    backtest.plot(dates, port_vals, assets, port_vals_SPY)\n",
    "    time8 = time.time()\n",
    "\n",
    "    df_results = backtest.get_strategy_results()\n",
    "    df_spy_results = backtest.get_spy_results()\n",
    "    \n",
    "    drawdowns = drawdown_plot(df_results, df_spy_results, PLOT_WIDTH, PLOT_HEIGHT, METHOD_NAME)\n",
    "    time9 = time.time()\n",
    "    rolling_returns, rolling_spy = get_rolling_returns(df_results, df_spy_results, YEARS_ROLLING_RETURNS, \n",
    "                                                       PLOT_WIDTH, PLOT_HEIGHT, METHOD_NAME)\n",
    "    time10 = time.time()\n",
    "\n",
    "    print(f'Dates: {dates[0].strftime(\"%B %d, %Y\")} to {dates[-1].strftime(\"%B %d, %Y\")}\\n')\n",
    "    \n",
    "    df = df.loc[(df.index >= dates[0]) & (df.index <= dates[-1])]\n",
    "\n",
    "    current_signal = df.iloc[-1]['State']\n",
    "    print(f'Current signal: {current_signal}')\n",
    "    \n",
    "    allocation_str = ''\n",
    "    for allocation in current_allocation[1:]:\n",
    "        allocation_str += f'\\n{allocation[1] * 100:.2f}% {allocation[0]}'\n",
    "        \n",
    "    print(f'Current allocation name: {current_allocation[0]}\\n')\n",
    "    print(f'{current_allocation[0]} allocation consists of: {allocation_str}\\n')\n",
    "\n",
    "    years = (dates[-1] - dates[0]).total_seconds() / 31536000\n",
    "    swaps = df['State'].shift().bfill().ne(df['State']).sum() / years\n",
    "    print(f'Average number of swaps per year: {swaps:.3f}\\n')\n",
    "\n",
    "    for state in ['Risk On', 'Risk Mid', 'Risk Alt', 'Risk Off']:\n",
    "        risk_state_pct = ((df['State'].values == state).sum() / len(df)) * 100\n",
    "        lowercase_state = state.lower()\n",
    "        print(f'{\"Percent of time \" + lowercase_state + \":\":<25} {risk_state_pct:>10.3f}%')\n",
    "    \n",
    "    corr = np.corrcoef(pd.Series(port_vals).pct_change().dropna(), df['Close'].pct_change().dropna())[0][1]\n",
    "    print(f'\\nCorrelation of strategy returns to S&P 500: {corr:.3f}\\n')\n",
    "\n",
    "    print(f'Average {YEARS_ROLLING_RETURNS} year rolling return - Strategy: {rolling_returns.mean():8.3f}%' \\\n",
    "          + f'   SPY: {rolling_spy.mean():8.3f}%')\n",
    "    print(f'Minimum {YEARS_ROLLING_RETURNS} year rolling return - Strategy: {rolling_returns.min():8.3f}%' \\\n",
    "          + f'   SPY: {rolling_spy.min():8.3f}%')\n",
    "    print(f'Maximum {YEARS_ROLLING_RETURNS} year rolling return - Strategy: {rolling_returns.max():8.3f}%' \\\n",
    "          + f'   SPY: {rolling_spy.max():8.3f}%')\n",
    "    time11 = time.time()\n",
    "\n",
    "    if PRINT_TIMES:\n",
    "        print(f'\\n{\"Operation\":<28} {\"Time (seconds)\":>10}')\n",
    "        print(f'{\"Load S&P 500 data\":<28} {time2 - time1:>10.3f}')\n",
    "        print(f'{\"Load strategy data\":<28} {time3 - time2:>10.3f}')\n",
    "        print(f'{\"Calculate signals\":<28} {time4 - time3:>10.3f}')\n",
    "        print(f'{\"Plot signals\":<28} {time5 - time4:>10.3f}')\n",
    "        print(f'{\"Load precalculated dfs\":<28} {time6 - time5:>10.3f}')\n",
    "        print(f'{\"Simulate backtest\":<28} {time7 - time6:>10.3f}')\n",
    "        print(f'{\"Plot backtest\":<28} {time8 - time7:>10.3f}')\n",
    "        print(f'{\"Plot drawdowns\":<28} {time9 - time8:>10.3f}')\n",
    "        print(f'{\"Plot rolling returns\":<28} {time10 - time9:>10.3f}')\n",
    "        print(f'{\"Print statistics\":<28} {time11 - time10:>10.3f}')\n",
    "\n",
    "    return df, df_results, precalculated_dfs, drawdowns, rolling_returns, rolling_spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a54a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:15:39.283692Z",
     "start_time": "2023-09-02T01:15:39.268193Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# If not using git clone, download daily_us_market_data.csv and MLM_Index_Returns.xlsx\n",
    "# Put the path to those below\n",
    "EXTENDED_DATA_PATH = r'data/daily_us_market_data.csv'\n",
    "KMLM_DATA_PATH = r'data/MLM_Index_Returns.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d00220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:43:39.829496Z",
     "start_time": "2023-09-02T01:43:39.817396Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Strategy parameters\n",
    "SP_500_TICKER = 'SPY'                # Used if not using extended data\n",
    "USE_EXTENDED_DATA = True             # If true, use data located at EXTENDED_DATA_PATH, else use the above ticker\n",
    "USE_DAILY_TBILL = False              # If true, use daily t-bill data. If false, use monthly interpolated to daily.\n",
    "\n",
    "LOWER_VOL_CUTOFF = 0.14              # Lower cutoff for volatility\n",
    "HIGHER_VOL_CUTOFF = 0.24             # Higher cutoff for volatility\n",
    "\n",
    "TBILL_LOOKBACK_DAYS = 21             # Lookback to determine if t-bill rate is up or down\n",
    "MOMENTUM_LOOKBACK_DAYS = 252         # Lookback to determine if S&P 500 momentum is positive or negative\n",
    "SHORT_MOMENTUM_LOOKBACK_DAYS = 126   # Lookback to use for the shorter momentum (unused currently)\n",
    "VOLATILITY_LOOKBACK_DAYS = 21        # Lookback to determine volatility\n",
    "\n",
    "USE_SMA = True                       # Whether to use an SMA or pure momentum signal\n",
    "USE_LINEAR_REGRESSION = False        # Whether to use lin. reg. or momentum to determine if t-bill rate is up or down\n",
    "USE_STRICT_BOUNDARIES = False        # Whether to use strict bondaries for deciding the allocation\n",
    "\n",
    "EXCLUDE_VOL = False                  # Whether to exclude the use of volatility in deciding allocation\n",
    "EXCLUDE_MOM = False                  # Whether to exclude the use of momentum in deciding allocation\n",
    "\n",
    "# Plot parameters\n",
    "PLOT_DPI = 300                  # DPI to use for the plot (too high will cause lag)\n",
    "PLOT_WIDTH = 15                 # Width to use for the plot\n",
    "PLOT_HEIGHT = 5                 # Height to use for the plot\n",
    "PLOT_FONT = 'Source Serif Pro'  # Font to use for the plot. Font must be installed on your device\n",
    " \n",
    "SHOW_RISK_ON = True             # Whether to show the shaded region for risk on\n",
    "SHOW_RISK_MID = True            # Whether to show the shaded region for risk mid\n",
    "SHOW_RISK_ALT = True            # Whether to show the shaded region for risk alt\n",
    "SHOW_RISK_OFF = True            # Whether to show the shaded region for risk off\n",
    "\n",
    "# Backtest parameters\n",
    "BACKTEST_START_DATE = None               # Start date for the backtest. None does as far back as possible\n",
    "\n",
    "METHOD = 'standard'   # Method to use for the backtest\n",
    "METHOD_NAME = 'MFEA'                     # Name to use for the backtest plot\n",
    "\n",
    "USE_SSO = False                          # Whether to use SSO or SPY + UPRO for 2x leverage\n",
    "\n",
    "MONTHLY_CALCULATION = False              # False does a daily calculation, true does a monthly calculation\n",
    "REBALANCE = 'daily'                      # Frequency to rebalance. Currently can be daily, monthly, or never\n",
    "\n",
    "TRANSACTION_COST = 0.001                 # Transaction cost as a decimal. Applied only on buying\n",
    "\n",
    "BACKTEST_LINE_WIDTH = 3                  # Line width to use for the backtest plot\n",
    "\n",
    "# Other\n",
    "YEARS_ROLLING_RETURNS = 3     # Number of years to use for rolling return plot\n",
    "\n",
    "USE_MODIFIED_SP_500 = False   # Use modified S&P 500. Will apply over all of the data, not just what appears in backtest\n",
    "MODIFIED_SP_500_CAGR = 7      # CAGR for the modified S&P 500 to have\n",
    "MODIFIED_SP_500_VOL = 0.2     # volatility for the modified S&P 500 to have\n",
    "\n",
    "PRINT_TIMES = False           # Prints times for diagnostic purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d1bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:44:25.066115Z",
     "start_time": "2023-09-02T01:44:18.651111Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df, df_results, precalculated_dfs, drawdowns, rolling_returns, rolling_returns_spy = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27ffc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-02T01:46:01.671037Z",
     "start_time": "2023-09-02T01:45:59.146445Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sim_results, dates = simulate_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098b01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
